{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final finished with best results but lack the ability to generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision.models import resnet50\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "import open_clip\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "import gc\n",
    "from transformers import BertConfig\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [55:42<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_augmentations(image_path, save_dir, num_augmentations):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    \n",
    "    transforms = {\n",
    "        'clahe': A.CLAHE(clip_limit=2.0, p=1.0),\n",
    "        'contrast': A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "        'flip': A.HorizontalFlip(p=1.0),\n",
    "        'gamma': A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "        'rotate': A.Rotate(limit=30, p=1.0)\n",
    "    }\n",
    "    \n",
    "    filename = Path(image_path).stem\n",
    "    \n",
    "    for i in range(num_augmentations):\n",
    "        selected_transforms = random.sample(list(transforms.items()), k=random.randint(1, 3))\n",
    "        \n",
    "        img_aug = img.copy()\n",
    "        aug_name = []\n",
    "        for name, transform in selected_transforms:\n",
    "            img_aug = transform(image=img_aug)['image']\n",
    "            aug_name.append(name)\n",
    "            \n",
    "        save_path = os.path.join(save_dir, f\"{filename}_{'_'.join(aug_name)}.jpg\")\n",
    "        cv2.imwrite(save_path, img_aug)\n",
    "\n",
    "def create_augmented_dataset(original_dir, target_size, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    original_images = list(Path(original_dir).glob('*.jpg'))\n",
    "    num_originals = len(original_images)\n",
    "    augs_per_image = (target_size - num_originals) // num_originals\n",
    "    \n",
    "    for img_path in tqdm(original_images):\n",
    "        # Copy original\n",
    "        img_name = img_path.name\n",
    "        cv2.imwrite(os.path.join(output_dir, img_name), cv2.imread(str(img_path)))\n",
    "        \n",
    "        # Create augmentations\n",
    "        create_augmentations(img_path, output_dir, augs_per_image)\n",
    "    \n",
    "    print(f\"Created {target_size} images\")\n",
    "\n",
    "# Usage\n",
    "create_augmented_dataset(\n",
    "    original_dir=r\"D:\\DMID\\24522883\\DICOM Export\",\n",
    "    target_size=5000,\n",
    "    output_dir=r\"D:\\DMID\\24522883\\DICOM-Augmented-Large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking reports folder: D:\\DMID\\24522883\\Reports\\Reports\n",
      "Available reports: ['Img001.txt', 'Img002.txt', 'Img003.txt', 'Img004.txt', 'Img005.txt', 'Img006.txt', 'Img007.txt', 'Img008.txt', 'Img009.txt', 'Img010.txt', 'Img011.txt', 'Img012.txt', 'Img013.txt', 'Img014.txt', 'Img015.txt', 'Img016.txt', 'Img017.txt', 'Img018.txt', 'Img019.txt', 'Img020.txt', 'Img021.txt', 'Img022.txt', 'Img023.txt', 'Img024.txt', 'Img025.txt', 'Img026.txt', 'Img027.txt', 'Img028.txt', 'Img029.txt', 'Img030.txt', 'Img031.txt', 'Img032.txt', 'Img033.txt', 'Img034.txt', 'Img035.txt', 'Img036.txt', 'Img037.txt', 'Img038.txt', 'Img039.txt', 'Img040.txt', 'Img041.txt', 'Img042.txt', 'Img043.txt', 'Img044.txt', 'Img045.txt', 'Img046.txt', 'Img047.txt', 'Img048.txt', 'Img049.txt', 'Img050.txt', 'Img051.txt', 'Img052.txt', 'Img053.txt', 'Img054.txt', 'Img055.txt', 'Img056.txt', 'Img057.txt', 'Img058.txt', 'Img059.txt', 'Img060.txt', 'Img061.txt', 'Img062.txt', 'Img063.txt', 'Img064.txt', 'Img065.txt', 'Img066.txt', 'Img067.txt', 'Img068.txt', 'Img069.txt', 'Img070.txt', 'Img071.txt', 'Img072.txt', 'Img073.txt', 'Img074.txt', 'Img075.txt', 'Img076.txt', 'Img077.txt', 'Img078.txt', 'Img079.txt', 'Img080.txt', 'Img081.txt', 'Img082.txt', 'Img083.txt', 'Img084.txt', 'Img085.txt', 'Img086.txt', 'Img087.txt', 'Img088.txt', 'Img089.txt', 'Img090.txt', 'Img091.txt', 'Img092.txt', 'Img093.txt', 'Img094.txt', 'Img095.txt', 'Img096.txt', 'Img097.txt', 'Img098.txt', 'Img099.txt', 'Img100.txt', 'Img101.txt', 'Img102.txt', 'Img103.txt', 'Img104.txt', 'Img105.txt', 'Img106.txt', 'Img107.txt', 'Img108.txt', 'Img109.txt', 'Img110.txt', 'Img111.txt', 'Img112.txt', 'Img113.txt', 'Img114.txt', 'Img115.txt', 'Img116.txt', 'Img117.txt', 'Img118.txt', 'Img119.txt', 'Img120.txt', 'Img121.txt', 'Img122.txt', 'Img123.txt', 'Img124.txt', 'Img125.txt', 'Img126.txt', 'Img127.txt', 'Img128.txt', 'Img129.txt', 'Img130.txt', 'Img131.txt', 'Img132.txt', 'Img133.txt', 'Img134.txt', 'Img135.txt', 'Img136.txt', 'Img137.txt', 'Img138.txt', 'Img139.txt', 'Img140.txt', 'Img141.txt', 'Img142.txt', 'Img143.txt', 'Img144.txt', 'Img145.txt', 'Img146.txt', 'Img147.txt', 'Img148.txt', 'Img149.txt', 'Img150.txt', 'Img151.txt', 'Img152.txt', 'Img153.txt', 'Img154.txt', 'Img155.txt', 'Img156.txt', 'Img157.txt', 'Img158.txt', 'Img159.txt', 'Img160.txt', 'Img161.txt', 'Img162.txt', 'Img163.txt', 'Img164.txt', 'Img165.txt', 'Img166.txt', 'Img167.txt', 'Img168.txt', 'Img169.txt', 'Img170.txt', 'Img171.txt', 'Img172.txt', 'Img173.txt', 'Img174.txt', 'Img175.txt', 'Img176.txt', 'Img177.txt', 'Img178.txt', 'Img179.txt', 'Img180.txt', 'Img181.txt', 'Img182.txt', 'Img183.txt', 'Img184.txt', 'Img185.txt', 'Img186.txt', 'Img187.txt', 'Img188.txt', 'Img189.txt', 'Img190.txt', 'Img191.txt', 'Img192.txt', 'Img193.txt', 'Img194.txt', 'Img195.txt', 'Img196.txt', 'Img197.txt', 'Img198.txt', 'Img199.txt', 'Img200.txt', 'Img201.txt', 'Img202.txt', 'Img203.txt', 'Img204.txt', 'Img205.txt', 'Img206.txt', 'Img207.txt', 'Img208.txt', 'Img209.txt', 'Img210.txt', 'Img211.txt', 'Img212.txt', 'Img213.txt', 'Img214.txt', 'Img215.txt', 'Img216.txt', 'Img217.txt', 'Img218.txt', 'Img219.txt', 'Img220.txt', 'Img221.txt', 'Img222.txt', 'Img223.txt', 'Img224.txt', 'Img225.txt', 'Img226.txt', 'Img227.txt', 'Img228.txt', 'Img229.txt', 'Img230.txt', 'Img231.txt', 'Img232.txt', 'Img233.txt', 'Img234.txt', 'Img235.txt', 'Img236.txt', 'Img237.txt', 'Img238.txt', 'Img239.txt', 'Img240.txt', 'Img241.txt', 'Img242.txt', 'Img243.txt', 'Img244.txt', 'Img245.txt', 'Img246.txt', 'Img247.txt', 'Img248.txt', 'Img249.txt', 'Img250.txt', 'Img251.txt', 'Img252.txt', 'Img253.txt', 'Img254.txt', 'Img255.txt', 'Img256.txt', 'Img257.txt', 'Img258.txt', 'Img259.txt', 'Img260.txt', 'Img261.txt', 'Img262.txt', 'Img263.txt', 'Img264.txt', 'Img265.txt', 'Img266.txt', 'Img267.txt', 'Img268.txt', 'Img269.txt', 'Img270.txt', 'Img271.txt', 'Img272.txt', 'Img273.txt', 'Img274.txt', 'Img275.txt', 'Img276.txt', 'Img277.txt', 'Img278.txt', 'Img279.txt', 'Img280.txt', 'Img281.txt', 'Img282.txt', 'Img283.txt', 'Img284.txt', 'Img285.txt', 'Img286.txt', 'Img287.txt', 'Img288.txt', 'Img289.txt', 'Img290.txt', 'Img291.txt', 'Img292.txt', 'Img293.txt', 'Img294.txt', 'Img295.txt', 'Img296.txt', 'Img297.txt', 'Img298.txt', 'Img299.txt', 'Img300.txt', 'Img301.txt', 'Img302.txt', 'Img303.txt', 'Img304.txt', 'Img305.txt', 'Img306.txt', 'Img307.txt', 'Img308.txt', 'Img309.txt', 'Img310.txt', 'Img311.txt', 'Img312.txt', 'Img313.txt', 'Img314.txt', 'Img315.txt', 'Img316.txt', 'Img317.txt', 'Img318.txt', 'Img319.txt', 'Img320.txt', 'Img321.txt', 'Img322.txt', 'Img323.txt', 'Img324.txt', 'Img325.txt', 'Img326.txt', 'Img327.txt', 'Img328.txt', 'Img329.txt', 'Img330.txt', 'Img331.txt', 'Img332.txt', 'Img333.txt', 'Img334.txt', 'Img335.txt', 'Img336.txt', 'Img337.txt', 'Img338.txt', 'Img339.txt', 'Img340.txt', 'Img341.txt', 'Img342.txt', 'Img343.txt', 'Img344.txt', 'Img345.txt', 'Img346.txt', 'Img347.txt', 'Img348.txt', 'Img349.txt', 'Img350.txt', 'Img351.txt', 'Img352.txt', 'Img353.txt', 'Img354.txt', 'Img355.txt', 'Img356.txt', 'Img357.txt', 'Img358.txt', 'Img359.txt', 'Img360.txt', 'Img361.txt', 'Img362.txt', 'Img363.txt', 'Img364.txt', 'Img365.txt', 'Img366.txt', 'Img367.txt', 'Img368.txt', 'Img369.txt', 'Img370.txt', 'Img371.txt', 'Img372.txt', 'Img373.txt', 'Img374.txt', 'Img375.txt', 'Img376.txt', 'Img377.txt', 'Img378.txt', 'Img379.txt', 'Img380.txt', 'Img381.txt', 'Img382.txt', 'Img383.txt', 'Img384.txt', 'Img385.txt', 'Img386.txt', 'Img387.txt', 'Img388.txt', 'Img389.txt', 'Img390.txt', 'Img391.txt', 'Img392.txt', 'Img393.txt', 'Img394.txt', 'Img395.txt', 'Img396.txt', 'Img397.txt', 'Img398.txt', 'Img399.txt', 'Img400.txt', 'Img401.txt', 'Img402.txt', 'Img403.txt', 'Img404.txt', 'Img405.txt', 'Img406.txt', 'Img407.txt', 'Img408.txt', 'Img409.txt', 'Img410.txt', 'Img411.txt', 'Img412.txt', 'Img413.txt', 'Img414.txt', 'Img415.txt', 'Img416.txt', 'Img417.txt', 'Img418.txt', 'Img419.txt', 'Img420.txt', 'Img421.txt', 'Img422.txt', 'Img423.txt', 'Img424.txt', 'Img425.txt', 'Img426.txt', 'Img427.txt', 'Img428.txt', 'Img429.txt', 'Img430.txt', 'Img431.txt', 'Img432.txt', 'Img433.txt', 'Img434.txt', 'Img435.txt', 'Img436.txt', 'Img437.txt', 'Img438.txt', 'Img439.txt', 'Img440.txt', 'Img441.txt', 'Img442.txt', 'Img443.txt', 'Img444.txt', 'Img445.txt', 'Img446.txt', 'Img447.txt', 'Img448.txt', 'Img449.txt', 'Img450.txt', 'Img451.txt', 'Img452.txt', 'Img453.txt', 'Img454.txt', 'Img455.txt', 'Img456.txt', 'Img457.txt', 'Img458.txt', 'Img459.txt', 'Img460.txt', 'Img461.txt', 'Img462.txt', 'Img463.txt', 'Img464.txt', 'Img465.txt', 'Img466.txt', 'Img467.txt', 'Img468.txt', 'Img469.txt', 'Img470.txt', 'Img471.txt', 'Img472.txt', 'Img473.txt', 'Img474.txt', 'Img475.txt', 'Img476.txt', 'Img477.txt', 'Img478.txt', 'Img479.txt', 'Img480.txt', 'Img481.txt', 'Img482.txt', 'Img483.txt', 'Img484.txt', 'Img485.txt', 'Img486.txt', 'Img487.txt', 'Img488.txt', 'Img489.txt', 'Img490.txt', 'Img491.txt', 'Img492.txt', 'Img493.txt', 'Img494.txt', 'Img495.txt', 'Img496.txt', 'Img497.txt', 'Img498.txt', 'Img499.txt', 'Img500.txt', 'Img501.txt', 'Img502.txt', 'Img503.txt', 'Img504.txt', 'Img505.txt', 'Img506.txt', 'Img507.txt', 'Img508.txt', 'Img509.txt', 'Img510.txt']\n",
      "Sample report path: D:\\DMID\\24522883\\Reports\\Reports\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "\n",
    "# First, let's verify report exists\n",
    "import os\n",
    "\n",
    "# Debug helper\n",
    "def print_report_info(report_folder):\n",
    "    print(\"Available reports:\", os.listdir(report_folder))\n",
    "    print(\"Sample report path:\", os.path.join(report_folder))\n",
    "\n",
    "# Modified dataset class\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, image_folder, report_folder, preprocess, tokenizer):\n",
    "        self.image_folder = image_folder\n",
    "        self.report_folder = report_folder  # Should be \"D:\\\\DMID\\\\24522883\\\\Reports\"\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Debug\n",
    "        print(\"Checking reports folder:\", self.report_folder)\n",
    "        print_report_info(self.report_folder)\n",
    "        \n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "        self.image_to_report = {}\n",
    "        \n",
    "        for img_name in self.image_files:\n",
    "            if '_' in img_name:  # Augmented image\n",
    "                num_str = img_name.split('_')[0].split('-')[1]\n",
    "            else:  # Original image\n",
    "                num_str = img_name.split('-')[1].split('.')[0]\n",
    "            \n",
    "            # Convert 5-digit to 3-digit (00071 -> 071)\n",
    "            num_str = num_str[-3:].zfill(3)\n",
    "            report_name = f\"Img{num_str}.txt\"\n",
    "            self.image_to_report[img_name] = report_name\n",
    "            \n",
    "            # Debug\n",
    "            if not os.path.exists(os.path.join(self.report_folder, report_name)):\n",
    "                print(f\"Warning: Missing report {report_name} for image {img_name}\")\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        report_name = self.image_to_report[img_name]\n",
    "        \n",
    "        image_path = os.path.join(self.image_folder, img_name)\n",
    "        report_path = os.path.join(self.report_folder, report_name)\n",
    "        \n",
    "        # Debug\n",
    "        if not os.path.exists(report_path):\n",
    "            print(f\"Error accessing: {report_path}\")\n",
    "            print(f\"From image: {img_name}\")\n",
    "            \n",
    "        image = self.preprocess(Image.open(image_path))\n",
    "        with open(report_path, 'r') as f:\n",
    "            text = f.read().strip()\n",
    "        \n",
    "        inputs = self.tokenizer(text, \n",
    "                              return_tensors=\"pt\", \n",
    "                              truncation=True, \n",
    "                              padding=\"max_length\", \n",
    "                              max_length=128)\n",
    "        \n",
    "        return image, inputs['input_ids'].squeeze(0)\n",
    "\n",
    "# Initialize with correct path\n",
    "augmented_dataset = AugmentedDataset(\n",
    "    image_folder=\"D:\\\\DMID\\\\24522883\\\\DICOM-Augmented-Large\",\n",
    "    report_folder=\"D:\\\\DMID\\\\24522883\\\\Reports\\\\Reports\",  # Remove double Reports\n",
    "    preprocess=preprocess,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "test_size = int(len(augmented_dataset) * 0.2)\n",
    "train_size = len(augmented_dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(augmented_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=0, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([32, 3, 224, 224]), Batch texts shape: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "for images, texts in train_loader:\n",
    "    print(f\"Batch images shape: {images.shape}, Batch texts shape: {texts.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "img, txt = train_dataset[211]\n",
    "print(img.shape, txt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4234\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__() + test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "class ContraLoss(nn.Module):\n",
    "    def __init__(self, temp=0.1):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, img_features, text_features):\n",
    "        # normalization\n",
    "        img_features = nn.functional.normalize(img_features + 1e-8, p=2, dim=1)\n",
    "        text_features = nn.functional.normalize(text_features + 1e-8, p=2, dim=1)\n",
    "        \n",
    "        # similarity with stability term\n",
    "        sim_matrix = torch.matmul(img_features, text_features.T)\n",
    "        sim_matrix = torch.clamp(sim_matrix, min=-1.0, max=1.0)\n",
    "        \n",
    "        # logits scalling by the temperature\n",
    "        logits = sim_matrix / self.temp\n",
    "        \n",
    "        labels = torch.arange(logits.size(0), device=logits.device)\n",
    "        \n",
    "        # stable loss\n",
    "        loss = (self.criterion(logits, labels) + self.criterion(logits.T, labels)) / 2\n",
    "        \n",
    "        # mean calculation\n",
    "        pos_sim = torch.diagonal(sim_matrix).mean()\n",
    "        \n",
    "        if torch.isnan(loss) or torch.isnan(pos_sim):\n",
    "            return None, None\n",
    "            \n",
    "        return loss, pos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__() + test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def train_model_02_wandb(model, train_data, test_data, lr=1e-4, epochs=20, device=\"cuda\", \n",
    "                        patience=5, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    config_dict = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"batch_size\": train_data.batch_size,\n",
    "        \"patience\": patience\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        wandb.init(project=\"medical-clip\", config=config_dict)\n",
    "        \n",
    "        criterion = ContraLoss(temp=0.1)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.2, betas=[0.9, 0.999])\n",
    "        \n",
    "        num_training_steps = len(train_data) * epochs\n",
    "        num_warmup_steps = 100\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=num_warmup_steps, \n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        max_grad_norm = 1.0\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [], 'train_sim': [],\n",
    "            'val_loss': [], 'val_sim': [],\n",
    "            'lr': [], 'gpu_memory': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            model.train()\n",
    "            train_losses, train_sims = [], []\n",
    "            \n",
    "            # Training loop with progress bar\n",
    "            train_pbar = tqdm(train_data, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "            for batch_idx, batch in enumerate(train_pbar):\n",
    "                images, texts = batch\n",
    "                images, texts = images.to(device), texts.to(device)\n",
    "                attn_mask = (texts > 0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                img_features, text_features = model(images, texts, attn_mask)\n",
    "                loss, sim = criterion(img_features, text_features)\n",
    "                \n",
    "                if loss is not None:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    train_losses.append(loss.item())\n",
    "                    train_sims.append(sim.item())\n",
    "                    \n",
    " \n",
    "                    train_pbar.set_postfix({\n",
    "                        'loss': f'{loss.item():.4f}',\n",
    "                        'sim': f'{sim.item():.4f}',\n",
    "                        'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                        'gpu_mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'\n",
    "                    })\n",
    "                \n",
    "                \n",
    "                wandb.log({\n",
    "                    'batch_loss': loss.item() if loss is not None else None,\n",
    "                    'batch_sim': sim.item() if sim is not None else None,\n",
    "                    'learning_rate': scheduler.get_last_lr()[0],\n",
    "                    'gpu_memory': torch.cuda.memory_allocated()/1e9\n",
    "                })\n",
    "            \n",
    "\n",
    "            model.eval()\n",
    "            val_losses, val_sims = [], []\n",
    "            \n",
    "            val_pbar = tqdm(test_data, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "            with torch.no_grad():\n",
    "                for batch in val_pbar:\n",
    "                    images, texts = batch\n",
    "                    images, texts = images.to(device), texts.to(device)\n",
    "                    attn_mask = (texts > 0).to(device)\n",
    "                    \n",
    "                    img_features, text_features = model(images, texts, attn_mask)\n",
    "                    loss, sim = criterion(img_features, text_features)\n",
    "                    \n",
    "                    if loss is not None:\n",
    "                        val_losses.append(loss.item())\n",
    "                        val_sims.append(sim.item())\n",
    "                        \n",
    "                        val_pbar.set_postfix({\n",
    "                            'loss': f'{loss.item():.4f}',\n",
    "                            'sim': f'{sim.item():.4f}'\n",
    "                        })\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            train_loss = np.mean(train_losses) if train_losses else float('nan')\n",
    "            train_sim = np.mean(train_sims) if train_sims else float('nan')\n",
    "            val_loss = np.mean(val_losses) if val_losses else float('nan')\n",
    "            val_sim = np.mean(val_sims) if val_sims else float('nan')\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            # Update history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_sim'].append(train_sim)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_sim'].append(val_sim)\n",
    "            history['lr'].append(scheduler.get_last_lr()[0])\n",
    "            history['gpu_memory'].append(torch.cuda.memory_allocated()/1e9)\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'train_sim': train_sim,\n",
    "                'val_loss': val_loss,\n",
    "                'val_sim': val_sim,\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Sim: {train_sim:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Sim: {val_sim:.4f}\")\n",
    "            print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.4e}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
    "            \n",
    "        # Modified checkpoint saving\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "            \n",
    "                checkpoint_path = os.path.join(checkpoint_dir, \"best_model_v02.pt\")\n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'train_loss': train_loss,\n",
    "                'val_sim': val_sim,\n",
    "                'train_sim': train_sim,\n",
    "                'history': history,\n",
    "                'config': config_dict  # Use config_dict instead of wandb.config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved best model checkpoint (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "                \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user. Saving checkpoint...\")\n",
    "            interrupt_path = os.path.join(checkpoint_dir, \"interrupted_model_v02.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history,\n",
    "                'config': config_dict  # Use config_dict here too\n",
    "            }, interrupt_path)\n",
    "            print(f\"Saved interrupted model to {interrupt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        raise e\n",
    "        \n",
    "    finally:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train')\n",
    "        plt.plot(history['val_loss'], label='Val')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['train_sim'], label='Train')\n",
    "        plt.plot(history['val_sim'], label='Val')\n",
    "        plt.title('Similarity')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"learning_curves\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "        \n",
    "        wandb.finish()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {total_time/60:.2f} minutes\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f} (Epoch {best_epoch+1})\")\n",
    "        print(f\"Model checkpoints saved in {checkpoint_dir}/\")\n",
    "        \n",
    "    return history, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CLIPv02(nn.Module):\n",
    "#     def __init__(self, embed_dim):\n",
    "#         super().__init__()\n",
    "#         # Image encoder\n",
    "#         self.image_encoder = resnet50(pretrained=True)  # Use pretrained weights\n",
    "#         self.image_encoder.fc = nn.Sequential(\n",
    "#             nn.Linear(self.image_encoder.fc.in_features, embed_dim*2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(embed_dim*2, embed_dim),\n",
    "#             nn.LayerNorm(embed_dim),\n",
    "#             nn.Dropout(p=0.2)\n",
    "#         )\n",
    "        \n",
    "#         # Text encoder\n",
    "#         bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "#         bert_config.layer_norm_params_type = \"default\"\n",
    "#         self.text_encoder = BertModel.from_pretrained(\n",
    "#             \"bert-base-uncased\",\n",
    "#             config=bert_config\n",
    "#         )\n",
    "        \n",
    "#         # Projection heads\n",
    "#         self.text_proj = nn.Sequential(\n",
    "#             nn.Linear(self.text_encoder.config.hidden_size, embed_dim*2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(embed_dim*2, embed_dim),\n",
    "#             nn.LayerNorm(embed_dim),\n",
    "#             nn.Dropout(p=0.2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, image, input_ids, attn_mask):\n",
    "#         # Image features\n",
    "#         img_features = self.image_encoder(image)\n",
    "#         img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         # Text features \n",
    "#         text_features = self.text_encoder(input_ids, attn_mask).pooler_output\n",
    "#         text_features = self.text_proj(text_features)\n",
    "#         text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         return img_features, text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class CLIPv02(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        # Image encoder\n",
    "        self.image_encoder = resnet50(pretrained=True)\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(self.image_encoder.fc.in_features, embed_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(p=0.1)\n",
    "        )\n",
    "        self.image_encoder.fc = nn.Identity()  # cancel out ResNet's original FC layer\n",
    "\n",
    "        # Text encoder\n",
    "        bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        self.text_encoder = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", \n",
    "            config=bert_config\n",
    "        )\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size, embed_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(p=0.1)\n",
    "        )\n",
    "        \n",
    "        #TODO `Encode_Image` and `Decode_Image`\n",
    "\n",
    "    def forward(self, image, input_ids, attn_mask):\n",
    "        # Image features\n",
    "        img_features = self.image_encoder(image)  \n",
    "        img_features = self.image_proj(img_features)  # Project to embedding space\n",
    "        img_features = img_features / img_features.norm(dim=-1, keepdim=True) \n",
    "\n",
    "        # Text features\n",
    "        text_features = self.text_encoder(input_ids, attention_mask=attn_mask).last_hidden_state\n",
    "        text_features = text_features.mean(dim=1)  # Pool features\n",
    "        text_features = self.text_proj(text_features)  # Project to embedding space\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)  \n",
    "\n",
    "        return img_features, text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fef142114d44b2b2493e860a1174cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20 - Time: 4565.27s\n",
      "Train Loss: 3.3980, Sim: 0.0062\n",
      "Val Loss: 3.1977, Sim: 0.1605\n",
      "Learning Rate: 4.9851e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 3.1977)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f0f2d2ae6a4ad48b4168832995dbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6557f9fbc74c40878bee53ae716ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20 - Time: 4635.72s\n",
      "Train Loss: 2.6777, Sim: 0.2940\n",
      "Val Loss: 2.4283, Sim: 0.3476\n",
      "Learning Rate: 4.7228e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 2.4283)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abd8e5213ec4adbbb1094c245ab65fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0fb5dcf6c4402ba2f1ebb149567297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20 - Time: 8150.39s\n",
      "Train Loss: 1.5974, Sim: 0.4438\n",
      "Val Loss: 1.4510, Sim: 0.5159\n",
      "Learning Rate: 4.4604e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 1.4510)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7650dff7fa74a70aedcd69839141301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bd5d58f750497aa2f831c8032d44fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20 - Time: 5949.19s\n",
      "Train Loss: 0.8892, Sim: 0.5612\n",
      "Val Loss: 1.1000, Sim: 0.5887\n",
      "Learning Rate: 4.1980e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 1.1000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d670dba80c804ad185ffbdbe06aace8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5740ba336e9f468ba9c04cf782f3b84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20 - Time: 7325.75s\n",
      "Train Loss: 0.5441, Sim: 0.6418\n",
      "Val Loss: 0.8013, Sim: 0.6614\n",
      "Learning Rate: 3.9356e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.8013)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d20a50c9d647d481a292efa71f36e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b36b0fbb78490fbef0a9755039012f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20 - Time: 4583.94s\n",
      "Train Loss: 0.3969, Sim: 0.6856\n",
      "Val Loss: 0.6060, Sim: 0.7244\n",
      "Learning Rate: 3.6733e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.6060)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db567662bd4521bba8716d78327d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae7e0c1fe5b4254976df175ff1a769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20 - Time: 4631.60s\n",
      "Train Loss: 0.3143, Sim: 0.7133\n",
      "Val Loss: 0.5976, Sim: 0.7184\n",
      "Learning Rate: 3.4109e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.5976)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16675361f1e94c0bafbb1adf3a44d702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7d61f015764416b3bd5502198c75ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20 - Time: 3465.41s\n",
      "Train Loss: 0.2606, Sim: 0.7361\n",
      "Val Loss: 0.4788, Sim: 0.7532\n",
      "Learning Rate: 3.1485e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.4788)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee65916d279144429b7c193427d4b63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25db3e087bb043d381f3107145a49844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20 - Time: 3783.34s\n",
      "Train Loss: 0.2321, Sim: 0.7526\n",
      "Val Loss: 0.4194, Sim: 0.7753\n",
      "Learning Rate: 2.8861e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.4194)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47c88cdecaf49939d003fe20ad146f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8081f6188f3a4196af32643bbbf03765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20 - Time: 3354.73s\n",
      "Train Loss: 0.1955, Sim: 0.7642\n",
      "Val Loss: 0.3796, Sim: 0.7912\n",
      "Learning Rate: 2.6238e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.3796)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26100ac231446b28fc6ec09d425f122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b48b892c9a4f46929c37940f2d7b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20 - Time: 3170.86s\n",
      "Train Loss: 0.1811, Sim: 0.7747\n",
      "Val Loss: 0.3346, Sim: 0.8037\n",
      "Learning Rate: 2.3614e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.3346)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c561ec03644a5b8af3838c7e0e616c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2066fb4ba7c54d71b4cf45e0ebc33f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20 - Time: 3750.16s\n",
      "Train Loss: 0.1688, Sim: 0.7847\n",
      "Val Loss: 0.3410, Sim: 0.8077\n",
      "Learning Rate: 2.0990e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.3410)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905f092b2ae2446a8da6cfa65d2925ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5694117aef5d4680a1ba899c3e1435ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20 - Time: 4981.97s\n",
      "Train Loss: 0.1628, Sim: 0.7899\n",
      "Val Loss: 0.2951, Sim: 0.8245\n",
      "Learning Rate: 1.8366e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2951)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25903bed4ce247129b963f9d8cf689b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a15608b60b45209254869082c7088c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20 - Time: 4341.03s\n",
      "Train Loss: 0.1594, Sim: 0.7971\n",
      "Val Loss: 0.2934, Sim: 0.8259\n",
      "Learning Rate: 1.5743e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2934)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdba2c3f04eb4dfcb34a6e5a1f5f3223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d64eb457c748808d404f7be1ca7f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20 - Time: 5355.28s\n",
      "Train Loss: 0.1537, Sim: 0.8048\n",
      "Val Loss: 0.2749, Sim: 0.8333\n",
      "Learning Rate: 1.3119e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2749)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bead1b5013542e2ad5c69e9a6fb0912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23db3b83f44289826c5e294ffca5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20 - Time: 11032.35s\n",
      "Train Loss: 0.1288, Sim: 0.8109\n",
      "Val Loss: 0.2656, Sim: 0.8416\n",
      "Learning Rate: 1.0495e-05\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2656)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11a32adad08463db1cc710eed3531b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9faddcae12c42fb9e4cc24951192552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20 - Time: 5167.68s\n",
      "Train Loss: 0.1292, Sim: 0.8166\n",
      "Val Loss: 0.2555, Sim: 0.8460\n",
      "Learning Rate: 7.8713e-06\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2555)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c064fb24ba495d9f012429a80c808a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef533f4cbfc46d8afe529b9361d8ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20 - Time: 9792.27s\n",
      "Train Loss: 0.1362, Sim: 0.8218\n",
      "Val Loss: 0.2495, Sim: 0.8509\n",
      "Learning Rate: 5.2475e-06\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2495)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab62916e4603497facf6dad05da49c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8334eabcf90c48548acf7dbe636f8bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20 - Time: 9545.79s\n",
      "Train Loss: 0.1307, Sim: 0.8232\n",
      "Val Loss: 0.2438, Sim: 0.8531\n",
      "Learning Rate: 2.6238e-06\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2438)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258ef3003b344116befd81f8acfb43f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 [Train]:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b59b600159a42249702283cdf215918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20 [Val]:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20 - Time: 3465.52s\n",
      "Train Loss: 0.1343, Sim: 0.8278\n",
      "Val Loss: 0.2432, Sim: 0.8552\n",
      "Learning Rate: 0.0000e+00\n",
      "GPU Memory: 3.5GB\n",
      "Saved best model checkpoint (Val Loss: 0.2432)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73de0f36b71745d6b1b37dc837ee7bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.051 MB of 0.051 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▇▇▆▆▅▄▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_sim</td><td>▁▁▁▁▂▅▅▆▆▇▇▇▇▇████▇█████████████████████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time</td><td>▂▂▅▃▅▂▂▁▂▁▁▂▃▂▃█▃▇▇▁</td></tr><tr><td>gpu_memory</td><td>▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆████▇▇▇▇▇▆▆▆▆▆▄▄▁▂▂▁▂▂▂▂</td></tr><tr><td>learning_rate</td><td>▇██▇▇▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_sim</td><td>▁▃▅▆▆▇▇▇▇▇██████████</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_sim</td><td>▁▃▅▅▆▇▇▇▇▇▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.15847</td></tr><tr><td>batch_sim</td><td>0.82881</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_time</td><td>3465.51561</td></tr><tr><td>gpu_memory</td><td>3.53713</td></tr><tr><td>learning_rate</td><td>0</td></tr><tr><td>train_loss</td><td>0.1343</td></tr><tr><td>train_sim</td><td>0.82776</td></tr><tr><td>val_loss</td><td>0.24321</td></tr><tr><td>val_sim</td><td>0.85519</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-elevator-26</strong> at: <a href='https://wandb.ai/abdelrahman-ramadan2004-arab-academy-for-science-technol/medical-clip/runs/0zggj1mu' target=\"_blank\">https://wandb.ai/abdelrahman-ramadan2004-arab-academy-for-science-technol/medical-clip/runs/0zggj1mu</a><br/> View project at: <a href='https://wandb.ai/abdelrahman-ramadan2004-arab-academy-for-science-technol/medical-clip' target=\"_blank\">https://wandb.ai/abdelrahman-ramadan2004-arab-academy-for-science-technol/medical-clip</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241215_144737-0zggj1mu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 1856.64 minutes\n",
      "Best validation loss: 0.2432 (Epoch 20)\n",
      "Model checkpoints saved in checkpoints/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'train_loss': [3.398010105456946,\n",
       "   2.677684156399853,\n",
       "   1.5973658662921977,\n",
       "   0.8891989425668176,\n",
       "   0.544074851105798,\n",
       "   0.39690371687119863,\n",
       "   0.3143194614435142,\n",
       "   0.2605795476515338,\n",
       "   0.23213184653025754,\n",
       "   0.19546840543735702,\n",
       "   0.1810544211909456,\n",
       "   0.16880460417355006,\n",
       "   0.16276559985752376,\n",
       "   0.15936315548166913,\n",
       "   0.1536517023759068,\n",
       "   0.1287929427651864,\n",
       "   0.12916961075070332,\n",
       "   0.13618441885512955,\n",
       "   0.130703956995792,\n",
       "   0.13429906836025557],\n",
       "  'train_sim': [0.006160708423264606,\n",
       "   0.2939739301800728,\n",
       "   0.44376003967141203,\n",
       "   0.5612196506194349,\n",
       "   0.6418011115407044,\n",
       "   0.685576152689052,\n",
       "   0.7133260852885697,\n",
       "   0.736105610177202,\n",
       "   0.7526106052803543,\n",
       "   0.7642123142503342,\n",
       "   0.7746967116616806,\n",
       "   0.7846894927744595,\n",
       "   0.7899009720334467,\n",
       "   0.7970875422909575,\n",
       "   0.8047668860768372,\n",
       "   0.8108985665834175,\n",
       "   0.8166289391382685,\n",
       "   0.8218022202545742,\n",
       "   0.8231990669133529,\n",
       "   0.82776274377445],\n",
       "  'val_loss': [3.197738629800302,\n",
       "   2.428272503393668,\n",
       "   1.451041214995914,\n",
       "   1.1000054765630651,\n",
       "   0.8013447434813888,\n",
       "   0.6060132582982382,\n",
       "   0.5975761060361509,\n",
       "   0.4787621321501555,\n",
       "   0.4193938771883647,\n",
       "   0.379624687411167,\n",
       "   0.33462934306374303,\n",
       "   0.3410348566593947,\n",
       "   0.29509748573656436,\n",
       "   0.29335675526548316,\n",
       "   0.27494447374785386,\n",
       "   0.2655689495581168,\n",
       "   0.2554855931688238,\n",
       "   0.2494994788258164,\n",
       "   0.24381439001471908,\n",
       "   0.24320768702913215],\n",
       "  'val_sim': [0.16046170283246924,\n",
       "   0.3476246617458485,\n",
       "   0.5158603842611666,\n",
       "   0.5887003408537971,\n",
       "   0.6613988986721745,\n",
       "   0.724422691044984,\n",
       "   0.7183600531684028,\n",
       "   0.7532467268131398,\n",
       "   0.7753081034730982,\n",
       "   0.7912479793583905,\n",
       "   0.8036969701449076,\n",
       "   0.8077060580253601,\n",
       "   0.82450505539223,\n",
       "   0.8258918965304339,\n",
       "   0.833258589108785,\n",
       "   0.8415848833543284,\n",
       "   0.845951239267985,\n",
       "   0.8509141957318341,\n",
       "   0.8530880676375495,\n",
       "   0.85519203654042],\n",
       "  'lr': [4.9851485148514855e-05,\n",
       "   4.722772277227723e-05,\n",
       "   4.4603960396039605e-05,\n",
       "   4.198019801980198e-05,\n",
       "   3.935643564356436e-05,\n",
       "   3.6732673267326737e-05,\n",
       "   3.410891089108911e-05,\n",
       "   3.1485148514851487e-05,\n",
       "   2.886138613861386e-05,\n",
       "   2.623762376237624e-05,\n",
       "   2.3613861386138615e-05,\n",
       "   2.099009900990099e-05,\n",
       "   1.8366336633663368e-05,\n",
       "   1.5742574257425743e-05,\n",
       "   1.311881188118812e-05,\n",
       "   1.0495049504950495e-05,\n",
       "   7.871287128712872e-06,\n",
       "   5.2475247524752475e-06,\n",
       "   2.6237623762376237e-06,\n",
       "   0.0],\n",
       "  'gpu_memory': [3.522587648,\n",
       "   3.52697856,\n",
       "   3.527830528,\n",
       "   3.526847488,\n",
       "   3.527764992,\n",
       "   3.527830528,\n",
       "   3.52817664,\n",
       "   3.526145024,\n",
       "   3.52686592,\n",
       "   3.527980032,\n",
       "   3.52686592,\n",
       "   3.523898368,\n",
       "   3.527324672,\n",
       "   3.526454272,\n",
       "   3.527783424,\n",
       "   3.529708544,\n",
       "   3.528307712,\n",
       "   3.529290752,\n",
       "   3.527914496,\n",
       "   3.528569856]},\n",
       " 0.24320768702913215)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model02 = CLIPv02(1024).to(device)\n",
    "train_model_02_wandb(model02, train_loader, test_loader, lr=5e-5, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model02.state_dict(),\n",
    "    'model_config': {\n",
    "        'embed_dim': 1024,\n",
    "    }\n",
    "}, os.path.join(save_dir, 'clip_v02_final-AugL-Drop-03.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 46\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_embeddings\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[0;32m     44\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m extract_text_embeddings(\n\u001b[0;32m     45\u001b[0m     clip_inference\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mcaptions\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted embeddings shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def extract_text_embeddings(model, captions, batch_size=32):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(captions), batch_size)):\n",
    "        batch_captions = captions[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        tokens = tokenizer(\n",
    "            batch_captions,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            _, text_features = model(\n",
    "                None,\n",
    "                tokens['input_ids'],\n",
    "                tokens['attention_mask']\n",
    "            )\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "            all_embeddings.append(text_features.cpu())\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    text_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    # Save embeddings for reuse\n",
    "    torch.save(text_embeddings, 'text_embeddings.pt')\n",
    "    \n",
    "    return text_embeddings\n",
    "\n",
    "# Extract embeddings\n",
    "text_embeddings = extract_text_embeddings(\n",
    "    clip_inference.model,\n",
    "    train_dataset.captions\n",
    ")\n",
    "\n",
    "print(f\"Extracted embeddings shape: {text_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\595839735.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
      "  0%|          | 0/3388 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m clip_inference \u001b[38;5;241m=\u001b[39m CLIPInference(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/clip_v02_final-AugL-Drop-03.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Create text embeddings database with proper captions list\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mclip_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_captions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Using extracted captions list\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Test on single image\u001b[39;00m\n\u001b[0;32m     69\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDMID\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m24522883\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDICOM Export\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimg-00310-00001.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[72], line 24\u001b[0m, in \u001b[0;36mCLIPInference.extract_text_embeddings\u001b[1;34m(self, text_data, tokenizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(text_data):\n\u001b[1;32m---> 24\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     32\u001b[0m         _, text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     34\u001b[0m             tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     35\u001b[0m             tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     36\u001b[0m         )\n\u001b[0;32m     37\u001b[0m         text_embeddings\u001b[38;5;241m.\u001b[39mappend(F\u001b[38;5;241m.\u001b[39mnormalize(text_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3055\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 3114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3117\u001b[0m     )\n\u001b[0;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   3120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3123\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class CLIPInference:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def extract_image_embeddings(self, image_tensor):\n",
    "        with torch.no_grad():\n",
    "            image_features, _ = self.model(image_tensor, None, None)\n",
    "            return F.normalize(image_features, dim=-1)\n",
    "\n",
    "    def extract_text_embeddings(self, text_data, tokenizer):\n",
    "        text_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for text in tqdm(text_data):\n",
    "                tokens = tokenizer(\n",
    "                    text, \n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                _, text_features = self.model(\n",
    "                    None,\n",
    "                    tokens['input_ids'],\n",
    "                    tokens['attention_mask']\n",
    "                )\n",
    "                text_embeddings.append(F.normalize(text_features, dim=-1).cpu())\n",
    "        return torch.cat(text_embeddings)\n",
    "\n",
    "    def find_nearest_captions(self, image_embedding, text_embeddings, captions, k=5):\n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(\n",
    "            image_embedding.cpu().numpy(),\n",
    "            text_embeddings.cpu().numpy()\n",
    "        )[0]\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_k_idx = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        return [(captions[idx], similarities[idx]) for idx in top_k_idx]\n",
    "\n",
    "# Usage\n",
    "# First get captions from dataset\n",
    "train_captions = []\n",
    "for idx in range(len(train_dataset)):\n",
    "    _, caption = train_dataset[idx]  # Assuming dataset returns (image, caption) pairs\n",
    "    train_captions.append(caption)\n",
    "\n",
    "# Now extract embeddings\n",
    "clip_inference = CLIPInference(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "\n",
    "# Create text embeddings database with proper captions list\n",
    "text_embeddings = clip_inference.extract_text_embeddings(\n",
    "    train_captions,  # Using extracted captions list\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Test on single image\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    "\n",
    "image_embedding = clip_inference.extract_image_embeddings(image_tensor)\n",
    "matches = clip_inference.find_nearest_captions(\n",
    "    image_embedding,\n",
    "    text_embeddings,\n",
    "    train_captions  # Using same captions list\n",
    ")\n",
    "\n",
    "print(\"\\nNearest captions:\")\n",
    "for caption, similarity in matches:\n",
    "    print(f\"Similarity: {similarity:.3f} | Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few captions:\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131,  1180,  1136,  1129,  6454,  1852,  9505,\n",
      "          131,   170,  3321,  1218,  3393, 25338, 23601,  1906,  2991, 11769,\n",
      "        19905,  1562,  5336,  1103,  2072,  7209,  5401,  1344, 10108, 12477,\n",
      "         2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,   125,  1665,\n",
      "          114,   119,  1185, 22832, 17599,  7867,  6617, 11531,  1562,   119,\n",
      "        16516,  9871,  1116,   131,   125,  1665,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725,   176,  1931,  5552,  7209,\n",
      "        14247,  1424,  8992,  1918,   113,   170,  1665,  1197,   172,   114,\n",
      "          119, 16516,  9871,  1116,   131,   125,  1161,  9505,   131,   170,\n",
      "         1415,  1218,  3393,  2991, 11769, 19905,  1562,  1107,  1103,  2211,\n",
      "          186, 18413,  6922,  1104,  1103,  7209,  1114, 16201,  1158,  2241,\n",
      "         3528,  4777,  5996,  1822, 10108,  8241,  1988,   113, 16516,  9871,\n",
      "         1116,   125,  1161,   114,   119,  1185, 17599,  7867,  6617, 11531,\n",
      "         1562,   119,   185, 20302,  1348,  6484,  2691,  2999,   119,   170,\n",
      "         8745, 12576,  1183, 14372,  1110,  6320,  1562,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725, 24862,  1114,   176,  1931,\n",
      "         5552,  7209, 14247,  1424,  8992,  1918,   113,   170,  1665,  1197,\n",
      "          171,   114,  9505,   131,   170,  1415,  1218,   118,  3393, 12692,\n",
      "         2991, 11769, 19905,  1562,  1107,  1103, 16557,  7631,  1104,  1103,\n",
      "         3105,   186, 18413,  6922,  1104,  1103,  7209,  5996,   170,  1822,\n",
      "        10108, 12477,  2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,\n",
      "          125,  1161,   114,   119,  1374, 12089,   170,  8745, 12576,  1183,\n",
      "        15029,  1562,   119, 26181, 11368, 26557, 11019,  1233,  6617, 11531,\n",
      "         1562,   119,  2241,   117, 15070,   117,  1105,   185, 20302,  1348,\n",
      "         6484,  2845,  2999,   119, 16516,  9871,  1116,   131,   125,  1161,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725, 24862,  1105,   176,  1931,\n",
      "         5552,  7209, 14247,  1424,  8992,  1918,   113,   170,  1665,  1197,\n",
      "          173,   114,  9505,   131,  1126,  5178,   118,  3393,  2991, 11769,\n",
      "        19905,  1562,  1107,  1103, 16557,  7631,  1104,  1126,  5047,  1105,\n",
      "         6144,   186, 18413,  6922,  1104,  1103,  7209,  1114, 16201,  1158,\n",
      "         2241,  3528,  4777,  5996,  1930, 26181, 11368,  8241,  1988,   113,\n",
      "        16516,  9871,  1116,   124,   114,   119,  1185, 22832, 17599,  7867,\n",
      "         6617, 11531,  1562,   119, 16516,  9871,  1116,   131,   124,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725, 24862,  1105,  7648,   176,\n",
      "         1931,  5552,  7209, 14247,  1424,  8992,  1918,   113,   170,  1665,\n",
      "         1197,   172,   114, 16516,  9871,  1116,   131,   124,  9505,   131,\n",
      "         1218,   118,  3393, 13102,  2991, 11769, 19905,  5401, 26181, 11368,\n",
      "         8241,  1988, 16516,  9871,  1116,   118,   124,  1185,   170,  8745,\n",
      "        12576,  1183,  8050, 26601, 12233,  2241,  1105, 15070,   118,  1185,\n",
      "        22832,  1785,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\2970360126.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m train_captions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(caption) \u001b[38;5;28;01mfor\u001b[39;00m _, caption \u001b[38;5;129;01min\u001b[39;00m train_dataset]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Create text embeddings database\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mclip_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_captions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Test on single image\u001b[39;00m\n\u001b[0;32m     57\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDMID\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m24522883\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDICOM Export\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimg-00310-00001.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[74], line 35\u001b[0m, in \u001b[0;36mCLIPInference.extract_text_embeddings\u001b[1;34m(self, text_data, tokenizer, batch_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     27\u001b[0m         batch_texts,\n\u001b[0;32m     28\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 35\u001b[0m         _, text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m         text_embeddings\u001b[38;5;241m.\u001b[39mappend(F\u001b[38;5;241m.\u001b[39mnormalize(text_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(text_embeddings)\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mCLIPv02.forward\u001b[1;34m(self, image, input_ids, attn_mask)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, input_ids, attn_mask):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Image features\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_proj(img_features)  \u001b[38;5;66;03m# Project to embedding space\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     img_features \u001b[38;5;241m=\u001b[39m img_features \u001b[38;5;241m/\u001b[39m img_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "# First debug what we're getting from dataset\n",
    "print(\"First few captions:\")\n",
    "for i, (_, caption) in enumerate(train_dataset):\n",
    "    if i < 5:  # Print first 5 captions\n",
    "        print(f\"Type: {type(caption)}, Content: {caption}\")\n",
    "\n",
    "# Modified CLIPInference class with fixed text embedding extraction\n",
    "class CLIPInference:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def extract_text_embeddings(self, text_data, tokenizer, batch_size=32):\n",
    "        # Convert all items to strings if they aren't already\n",
    "        text_data = [str(text) for text in text_data]\n",
    "        text_embeddings = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(text_data), batch_size)):\n",
    "            batch_texts = text_data[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            tokens = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, text_features = self.model(\n",
    "                    None,\n",
    "                    tokens['input_ids'],\n",
    "                    tokens['attention_mask']\n",
    "                )\n",
    "                text_embeddings.append(F.normalize(text_features, dim=-1).cpu())\n",
    "        \n",
    "        return torch.cat(text_embeddings)\n",
    "\n",
    "# Usage\n",
    "clip_inference = CLIPInference(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "\n",
    "# Convert captions to strings explicitly\n",
    "train_captions = [str(caption) for _, caption in train_dataset]\n",
    "\n",
    "# Create text embeddings database\n",
    "text_embeddings = clip_inference.extract_text_embeddings(\n",
    "    train_captions,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Test on single image\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    "\n",
    "image_embedding = clip_inference.extract_image_embeddings(image_tensor)\n",
    "matches = clip_inference.find_nearest_captions(\n",
    "    image_embedding,\n",
    "    text_embeddings,\n",
    "    train_captions\n",
    ")\n",
    "\n",
    "print(\"\\nNearest captions:\")\n",
    "for caption, similarity in matches:\n",
    "    print(f\"Similarity: {similarity:.3f} | Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\3278582360.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
      "100%|██████████| 106/106 [03:34<00:00,  2.02s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m clip_inference\u001b[38;5;241m.\u001b[39mpreprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m image_embedding \u001b[38;5;241m=\u001b[39m clip_inference\u001b[38;5;241m.\u001b[39mextract_image_embeddings(image_tensor)\n\u001b[1;32m---> 77\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mclip_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_nearest_captions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_captions\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNearest captions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m caption, similarity \u001b[38;5;129;01min\u001b[39;00m matches:\n",
      "Cell \u001b[1;32mIn[75], line 59\u001b[0m, in \u001b[0;36mCLIPInference.find_nearest_captions\u001b[1;34m(self, image_embedding, text_embeddings, captions, k)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_nearest_captions\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_embedding, text_embeddings, captions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 59\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     60\u001b[0m     top_k_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(similarities, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:k]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(captions[idx], similarities[idx]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_k_idx]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "class CLIPInference:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract_text_embeddings(self, text_data, tokenizer, batch_size=32):\n",
    "        text_data = [str(text) for text in text_data]\n",
    "        text_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(text_data), batch_size)):\n",
    "            batch_texts = text_data[i:i + batch_size]\n",
    "            \n",
    "            tokens = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Create dummy image tensor of correct shape\n",
    "            dummy_image = torch.zeros(len(batch_texts), 3, 224, 224).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, text_features = self.model(\n",
    "                    dummy_image,\n",
    "                    tokens['input_ids'],\n",
    "                    tokens['attention_mask']\n",
    "                )\n",
    "                text_embeddings.append(F.normalize(text_features, dim=-1).cpu())\n",
    "        \n",
    "        return torch.cat(text_embeddings)\n",
    "\n",
    "    def extract_image_embeddings(self, image_tensor):\n",
    "        with torch.no_grad():\n",
    "            # Create dummy text inputs\n",
    "            dummy_tokens = {\n",
    "                'input_ids': torch.zeros(1, 1).long().to(self.device),\n",
    "                'attention_mask': torch.ones(1, 1).to(self.device)\n",
    "            }\n",
    "            \n",
    "            image_features, _ = self.model(\n",
    "                image_tensor,\n",
    "                dummy_tokens['input_ids'],\n",
    "                dummy_tokens['attention_mask']\n",
    "            )\n",
    "            return F.normalize(image_features, dim=-1)\n",
    "\n",
    "    def find_nearest_captions(self, image_embedding, text_embeddings, captions, k=5):\n",
    "        similarities = torch.matmul(image_embedding, text_embeddings.t())[0]\n",
    "        top_k_idx = torch.argsort(similarities, descending=True)[:k]\n",
    "        return [(captions[idx], similarities[idx].item()) for idx in top_k_idx]\n",
    "\n",
    "# Usage\n",
    "clip_inference = CLIPInference(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "train_captions = [str(caption) for _, caption in train_dataset]\n",
    "\n",
    "text_embeddings = clip_inference.extract_text_embeddings(\n",
    "    train_captions,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = clip_inference.preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "image_embedding = clip_inference.extract_image_embeddings(image_tensor)\n",
    "matches = clip_inference.find_nearest_captions(\n",
    "    image_embedding,\n",
    "    text_embeddings,\n",
    "    train_captions\n",
    ")\n",
    "\n",
    "print(\"\\nNearest captions:\")\n",
    "for caption, similarity in matches:\n",
    "    print(f\"Similarity: {similarity:.3f} | Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\185059842.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
      "100%|██████████| 106/106 [02:06<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest captions:\n",
      "Similarity: 0.156 | Caption: tensor([  101,  7209,  5239,   131,  8941, 20497, 12725, 24862,  7209, 14247,\n",
      "         1424,  8992,  1918,   113,   170,  1665,  1197,   118,   170,   114,\n",
      "          119, 16516,  9871,  1116,   131,   126,  9505,   131,   170,  1415,\n",
      "        12692,  2991, 11769, 19905,  1114,   170,   188, 20437,  8360,  7464,\n",
      "         1114,   175, 13335,  1182,  1104,  4422, 17599,  7867,  6617, 11531,\n",
      "         1116,  1562,  1107,  1103,  5047,   186, 18413,  6922,  1104,  1103,\n",
      "         7209,  1107,  8702,  6066,  7969, 16201,  1158,  2241,  5401,   170,\n",
      "        12477,  2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,   126,\n",
      "          114,   119, 15070,  2691,  2999,   119, 26181, 11368,   118, 26557,\n",
      "        11019,  1233,  6617, 11531,  1562,   119,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Similarity: 0.156 | Caption: tensor([  101,  7209,  5239,   131,  8941, 20497, 12725, 24862,  7209, 14247,\n",
      "         1424,  8992,  1918,   113,   170,  1665,  1197,   118,   170,   114,\n",
      "          119, 16516,  9871,  1116,   131,   126,  9505,   131,   170,  1415,\n",
      "        12692,  2991, 11769, 19905,  1114,   170,   188, 20437,  8360,  7464,\n",
      "         1114,   175, 13335,  1182,  1104,  4422, 17599,  7867,  6617, 11531,\n",
      "         1116,  1562,  1107,  1103,  2211,   186, 18413,  6922,  1104,  1103,\n",
      "         7209,  5401,   170, 12477,  2646, 15454,  8241,  1988,   113, 16516,\n",
      "         9871,  1116,   126,   114,   119,  2241,  1105, 15070,  2845,  2999,\n",
      "          119, 26181, 11368,   118, 26557, 11019,  1233,  6617, 11531,  1562,\n",
      "          119,  1374, 12089,   170,  8745, 12576,  1183, 15029,  1562,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Similarity: 0.156 | Caption: tensor([  101,  7209,  5239,   131,  8941, 20497, 12725, 24862,  7209, 14247,\n",
      "         1424,  8992,  1918,   113,   170,  1665,  1197,   118,   170,   114,\n",
      "          119, 16516,  9871,  1116,   131,   126,  9505,   131,   170,  1415,\n",
      "        12692,  2991, 11769, 19905,  1114,   170,   188, 20437,  8360,  7464,\n",
      "         1114,   175, 13335,  1182,  1104,  4422, 17599,  7867,  6617, 11531,\n",
      "         1116,  1562,  1107,  1103,  5047,   186, 18413,  6922,  1104,  1103,\n",
      "         7209,  1107,  8702,  6066,  7969, 16201,  1158,  2241,  5401,   170,\n",
      "        12477,  2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,   126,\n",
      "          114,   119, 15070,  2691,  2999,   119, 26181, 11368,   118, 26557,\n",
      "        11019,  1233,  6617, 11531,  1562,   119,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Similarity: 0.156 | Caption: tensor([  101,  7209,  5239,   131,  8941, 20497, 12725, 24862,  7209, 14247,\n",
      "         1424,  8992,  1918,   113,   170,  1665,  1197,   118,   170,   114,\n",
      "          119, 16516,  9871,  1116,   131,   126,  9505,   131,   170,  1415,\n",
      "        12692,  2991, 11769, 19905,  1114,   170,   188, 20437,  8360,  7464,\n",
      "         1114,   175, 13335,  1182,  1104,  4422, 17599,  7867,  6617, 11531,\n",
      "         1116,  1562,  1107,  1103,  2211,   186, 18413,  6922,  1104,  1103,\n",
      "         7209,  5401,   170, 12477,  2646, 15454,  8241,  1988,   113, 16516,\n",
      "         9871,  1116,   126,   114,   119,  2241,  1105, 15070,  2845,  2999,\n",
      "          119, 26181, 11368,   118, 26557, 11019,  1233,  6617, 11531,  1562,\n",
      "          119,  1374, 12089,   170,  8745, 12576,  1183, 15029,  1562,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Similarity: 0.156 | Caption: tensor([  101,  7209,  5239,   131,  8941, 20497, 12725, 24862,  7209, 14247,\n",
      "         1424,  8992,  1918,   113,   170,  1665,  1197,   118,   170,   114,\n",
      "          119, 16516,  9871,  1116,   131,   126,  9505,   131,   170,  1415,\n",
      "        12692,  2991, 11769, 19905,  1114,   170,   188, 20437,  8360,  7464,\n",
      "         1114,   175, 13335,  1182,  1104,  4422, 17599,  7867,  6617, 11531,\n",
      "         1116,  1562,  1107,  1103,  5047,   186, 18413,  6922,  1104,  1103,\n",
      "         7209,  1107,  8702,  6066,  7969, 16201,  1158,  2241,  5401,   170,\n",
      "        12477,  2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,   126,\n",
      "          114,   119, 15070,  2691,  2999,   119, 26181, 11368,   118, 26557,\n",
      "        11019,  1233,  6617, 11531,  1562,   119,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "class CLIPInference:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract_text_embeddings(self, text_data, tokenizer, batch_size=32):\n",
    "        text_data = [str(text) for text in text_data]\n",
    "        text_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(text_data), batch_size)):\n",
    "            batch_texts = text_data[i:i + batch_size]\n",
    "            \n",
    "            tokens = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            dummy_image = torch.zeros(len(batch_texts), 3, 224, 224).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, text_features = self.model(\n",
    "                    dummy_image,\n",
    "                    tokens['input_ids'],\n",
    "                    tokens['attention_mask']\n",
    "                )\n",
    "                text_embeddings.append(text_features.cpu())\n",
    "        \n",
    "        return torch.cat(text_embeddings, dim=0)\n",
    "\n",
    "    def extract_image_embeddings(self, image_tensor):\n",
    "        with torch.no_grad():\n",
    "            dummy_tokens = {\n",
    "                'input_ids': torch.zeros(1, 1).long().to(self.device),\n",
    "                'attention_mask': torch.ones(1, 1).to(self.device)\n",
    "            }\n",
    "            \n",
    "            image_features, _ = self.model(\n",
    "                image_tensor,\n",
    "                dummy_tokens['input_ids'],\n",
    "                dummy_tokens['attention_mask']\n",
    "            )\n",
    "            return image_features\n",
    "\n",
    "    def find_nearest_captions(self, image_embedding, text_embeddings, captions, k=5):\n",
    "        # Move tensors to same device\n",
    "        image_embedding = image_embedding.to(self.device)\n",
    "        text_embeddings = text_embeddings.to(self.device)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_embedding = F.normalize(image_embedding, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = torch.matmul(image_embedding, text_embeddings.t())[0]\n",
    "        top_k_idx = torch.argsort(similarities, descending=True)[:k]\n",
    "        \n",
    "        return [(captions[idx], similarities[idx].item()) for idx in top_k_idx]\n",
    "\n",
    "# Usage\n",
    "clip_inference = CLIPInference(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "train_captions = [str(caption) for _, caption in train_dataset]\n",
    "\n",
    "# Get text embeddings\n",
    "text_embeddings = clip_inference.extract_text_embeddings(\n",
    "    train_captions,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Process image\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = clip_inference.preprocess(image).unsqueeze(0).to(clip_inference.device)\n",
    "\n",
    "# Get image embedding\n",
    "image_embedding = clip_inference.extract_image_embeddings(image_tensor)\n",
    "\n",
    "# Find matches\n",
    "matches = clip_inference.find_nearest_captions(\n",
    "    image_embedding,\n",
    "    text_embeddings,\n",
    "    train_captions\n",
    ")\n",
    "\n",
    "print(\"\\nNearest captions:\")\n",
    "for caption, similarity in matches:\n",
    "    print(f\"Similarity: {similarity:.3f} | Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\185059842.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
      "100%|██████████| 106/106 [08:40<00:00,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest captions:\n",
      "Similarity: 0.090 | Caption: 101 7209 5239 131 20497 12725 24862 1105 7648 176 1931 5552 7209 14247 1424 8992 1918 113 170 1665 1197 171 114 16516 9871 1116 131 122 9505 131 1185 22832 2991 11769 19905 2241 1105 15070 118 1185 22832 1785 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Similarity: 0.090 | Caption: 101 7209 5239 131 20497 12725 24862 1105 7648 176 1931 5552 7209 14247 1424 8992 1918 113 170 1665 1197 171 114 16516 9871 1116 131 122 9505 131 1185 22832 2991 11769 19905 2241 1105 15070 118 1185 22832 1785 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Similarity: 0.090 | Caption: 101 7209 5239 131 20497 12725 24862 1105 7648 176 1931 5552 7209 14247 1424 8992 1918 113 170 1665 1197 171 114 16516 9871 1116 131 122 9505 131 1185 22832 2991 11769 19905 2241 1105 15070 118 1185 22832 1785 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Similarity: 0.090 | Caption: 101 7209 5239 131 20497 12725 24862 1105 7648 176 1931 5552 7209 14247 1424 8992 1918 113 170 1665 1197 171 114 16516 9871 1116 131 122 9505 131 1185 22832 2991 11769 19905 2241 1105 15070 118 1185 22832 1785 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Similarity: 0.090 | Caption: 101 7209 5239 131 20497 12725 24862 1105 7648 176 1931 5552 7209 14247 1424 8992 1918 113 170 1665 1197 171 114 16516 9871 1116 131 122 9505 131 1185 22832 2991 11769 19905 2241 1105 15070 118 1185 22832 1785 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "clip_inference = CLIPInference(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "\n",
    "# Clean and format captions properly\n",
    "train_captions = []\n",
    "for _, caption in train_dataset:\n",
    "    # Convert tensor to string if needed\n",
    "    if isinstance(caption, torch.Tensor):\n",
    "        caption = caption.numpy().tolist()  # Convert tensor to list\n",
    "    # Convert list/array to string if needed\n",
    "    if isinstance(caption, (list, np.ndarray)):\n",
    "        caption = ' '.join(map(str, caption))\n",
    "    train_captions.append(str(caption))\n",
    "\n",
    "# Get text embeddings\n",
    "text_embeddings = clip_inference.extract_text_embeddings(\n",
    "    train_captions,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Process image\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image_tensor = clip_inference.preprocess(image).unsqueeze(0).to(clip_inference.device)\n",
    "\n",
    "# Get image embedding\n",
    "image_embedding = clip_inference.extract_image_embeddings(image_tensor)\n",
    "\n",
    "# Find matches\n",
    "matches = clip_inference.find_nearest_captions(\n",
    "    image_embedding,\n",
    "    text_embeddings,\n",
    "    train_captions\n",
    ")\n",
    "\n",
    "print(\"\\nNearest captions:\")\n",
    "for caption, similarity in matches:\n",
    "    print(f\"Similarity: {similarity:.3f} | Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few captions:\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131,  1180,  1136,  1129,  6454,  1852,  9505,\n",
      "          131,   170,  3321,  1218,  3393, 25338, 23601,  1906,  2991, 11769,\n",
      "        19905,  1562,  5336,  1103,  2072,  7209,  5401,  1344, 10108, 12477,\n",
      "         2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,   125,  1665,\n",
      "          114,   119,  1185, 22832, 17599,  7867,  6617, 11531,  1562,   119,\n",
      "        16516,  9871,  1116,   131,   125,  1665,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725,   176,  1931,  5552,  7209,\n",
      "        14247,  1424,  8992,  1918,   113,   170,  1665,  1197,   172,   114,\n",
      "          119, 16516,  9871,  1116,   131,   125,  1161,  9505,   131,   170,\n",
      "         1415,  1218,  3393,  2991, 11769, 19905,  1562,  1107,  1103,  2211,\n",
      "          186, 18413,  6922,  1104,  1103,  7209,  1114, 16201,  1158,  2241,\n",
      "         3528,  4777,  5996,  1822, 10108,  8241,  1988,   113, 16516,  9871,\n",
      "         1116,   125,  1161,   114,   119,  1185, 17599,  7867,  6617, 11531,\n",
      "         1562,   119,   185, 20302,  1348,  6484,  2691,  2999,   119,   170,\n",
      "         8745, 12576,  1183, 14372,  1110,  6320,  1562,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725, 24862,  1114,   176,  1931,\n",
      "         5552,  7209, 14247,  1424,  8992,  1918,   113,   170,  1665,  1197,\n",
      "          171,   114,  9505,   131,   170,  1415,  1218,   118,  3393, 12692,\n",
      "         2991, 11769, 19905,  1562,  1107,  1103, 16557,  7631,  1104,  1103,\n",
      "         3105,   186, 18413,  6922,  1104,  1103,  7209,  5996,   170,  1822,\n",
      "        10108, 12477,  2646, 15454,  8241,  1988,   113, 16516,  9871,  1116,\n",
      "          125,  1161,   114,   119,  1374, 12089,   170,  8745, 12576,  1183,\n",
      "        15029,  1562,   119, 26181, 11368, 26557, 11019,  1233,  6617, 11531,\n",
      "         1562,   119,  2241,   117, 15070,   117,  1105,   185, 20302,  1348,\n",
      "         6484,  2845,  2999,   119, 16516,  9871,  1116,   131,   125,  1161,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725, 24862,  1105,   176,  1931,\n",
      "         5552,  7209, 14247,  1424,  8992,  1918,   113,   170,  1665,  1197,\n",
      "          173,   114,  9505,   131,  1126,  5178,   118,  3393,  2991, 11769,\n",
      "        19905,  1562,  1107,  1103, 16557,  7631,  1104,  1126,  5047,  1105,\n",
      "         6144,   186, 18413,  6922,  1104,  1103,  7209,  1114, 16201,  1158,\n",
      "         2241,  3528,  4777,  5996,  1930, 26181, 11368,  8241,  1988,   113,\n",
      "        16516,  9871,  1116,   124,   114,   119,  1185, 22832, 17599,  7867,\n",
      "         6617, 11531,  1562,   119, 16516,  9871,  1116,   131,   124,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Type: <class 'torch.Tensor'>, Content: tensor([  101,  7209,  5239,   131, 20497, 12725, 24862,  1105,  7648,   176,\n",
      "         1931,  5552,  7209, 14247,  1424,  8992,  1918,   113,   170,  1665,\n",
      "         1197,   172,   114, 16516,  9871,  1116,   131,   124,  9505,   131,\n",
      "         1218,   118,  3393, 13102,  2991, 11769, 19905,  5401, 26181, 11368,\n",
      "         8241,  1988, 16516,  9871,  1116,   118,   124,  1185,   170,  8745,\n",
      "        12576,  1183,  8050, 26601, 12233,  2241,  1105, 15070,   118,  1185,\n",
      "        22832,  1785,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst few captions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Print first 5 captions\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mType: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, Content: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcaption\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 59\u001b[0m, in \u001b[0;36mAugmentedDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError accessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(report_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     61\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2293\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2290\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreducing_gap must be 1.0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 2293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2295\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"First few captions:\")\n",
    "for i, (_, caption) in enumerate(train_dataset):\n",
    "    if i < 5:  # Print first 5 captions\n",
    "        print(f\"Type: {type(caption)}, Content: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CLIPv02Inference:\n",
    "    def __init__(self, model_path, device=device):\n",
    "        self.device = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        checkpoints = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoints['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    def get_similarity(self, image_path, text):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = tokens['input_ids'].to(self.device)\n",
    "        attn_mask = tokens['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            img_features, text_features = self.model(\n",
    "                image_tensor,\n",
    "                input_ids,\n",
    "                attn_mask\n",
    "            )\n",
    "            img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = torch.matmul(img_features,text_features.T)\n",
    "        return similarity.item()\n",
    "    def best_match(self, image_path, text_can):\n",
    "        similarities = []\n",
    "        for text in text_can:\n",
    "            score = self.get_similarity(image_path, text)\n",
    "            similarities.append((text, score))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\2328808801.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load(model_path, map_location=self.device)\n",
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mtest_clip_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 88\u001b[0m, in \u001b[0;36mtest_clip_inference\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDMID\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m24522883\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDICOM Export\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimg-00310-00001.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m candidates \u001b[38;5;241m=\u001b[39m vocabulary[:\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m---> 88\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mclip_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_best_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContrastive matches:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, score \u001b[38;5;129;01min\u001b[39;00m matches:\n",
      "Cell \u001b[1;32mIn[52], line 71\u001b[0m, in \u001b[0;36mCLIPv02Inference.find_best_matches\u001b[1;34m(self, image_path, text_candidates, top_k)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_best_matches\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path, text_candidates, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Get normalized similarity scores\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Sort by score\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[52], line 27\u001b[0m, in \u001b[0;36mCLIPv02Inference.get_similarity\u001b[1;34m(self, image_path, text_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Process all texts as a batch\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Get features\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     img_features, text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m     38\u001b[0m         image_tensor,\n\u001b[0;32m     39\u001b[0m         tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     40\u001b[0m         tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3055\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 3114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3117\u001b[0m     )\n\u001b[0;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   3120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3123\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "class CLIPv02Inference:\n",
    "    def __init__(self, model_path, device=None, temperature=0.07):\n",
    "        self.temperature = temperature\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        \n",
    "        # Load model\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        checkpoints = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoints['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def get_similarity(self, image_path, text_list):\n",
    "        \"\"\"Calculate contrastive similarity scores between image and text list\"\"\"\n",
    "        # Process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Process all texts as a batch\n",
    "        tokens = self.tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get features\n",
    "            img_features, text_features = self.model(\n",
    "                image_tensor,\n",
    "                tokens['input_ids'],\n",
    "                tokens['attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Normalize features\n",
    "            img_features = F.normalize(img_features, dim=-1)\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "            \n",
    "            # Calculate similarity matrix\n",
    "            logits = torch.matmul(img_features, text_features.T) / self.temperature\n",
    "            \n",
    "            # Calculate contrastive probabilities\n",
    "            probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            \n",
    "            # Get individual scores using cross-entropy normalization\n",
    "            scores = []\n",
    "            for i, text in enumerate(text_list):\n",
    "                # Calculate positive pair score\n",
    "                pos_score = logits[0, i]\n",
    "                \n",
    "                # Calculate negative pairs score (other texts)\n",
    "                neg_indices = [j for j in range(len(text_list)) if j != i]\n",
    "                neg_scores = logits[0, neg_indices]\n",
    "                \n",
    "                # Normalized score using positive and negative pairs\n",
    "                normalized_score = (pos_score / (pos_score + neg_scores.sum())).item()\n",
    "                scores.append((text, normalized_score))\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def find_best_matches(self, image_path, text_candidates, top_k=5):\n",
    "        # Get normalized similarity scores\n",
    "        similarities = self.get_similarity(image_path, text_candidates)\n",
    "        \n",
    "        # Sort by score\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "\n",
    "# Usage example\n",
    "def test_clip_inference():\n",
    "    clip_inference = CLIPv02Inference(\n",
    "        model_path=\"models/clip_v02_final-AugL-Drop-03.pth\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "    candidates = [\"fibro\", \"fatty\", \"glandular\", \"breast parenchyma\", \"and\",'pizza']\n",
    "    \n",
    "    matches = clip_inference.find_best_matches(image_path, candidates)\n",
    "    print(\"\\nContrastive matches:\")\n",
    "    for text, score in matches:\n",
    "        print(f\"Score: {score:.3f} | Text: {text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_clip_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\661377589.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/clip_v02_final-AugL-Drop-03.pth\"\n",
    "clip_inference = CLIPv02Inference(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['23' '23rd' '4a' '4b' '4c' '4cno' 'abnormal' 'abnormality'\n",
      " 'abovementioned' 'acr' 'acra' 'acrb' 'acrc' 'acrd' 'adenopathies'\n",
      " 'adenopathy' 'adjacent' 'amorphous' 'another' 'anterior' 'appear'\n",
      " 'appearance' 'appears' 'architectural' 'area' 'areolar' 'aspect'\n",
      " 'aspects' 'asymetric' 'asymmetric' 'axilla' 'axillary' 'beingn' 'benign'\n",
      " 'benignlooking' 'benignvascular' 'benin' 'bening' 'beninglooking'\n",
      " 'biards' 'bifid' 'birads' 'birads3' 'birads4a' 'birads4b' 'birads4c'\n",
      " 'birads5' 'branching' 'breast' 'breastbirads' 'brest' 'calcification'\n",
      " 'calcifications' 'calcified' 'causing' 'central' 'chemoport' 'coarse'\n",
      " 'commented' 'composition' 'compositions' 'correlation' 'could' 'defied'\n",
      " 'defined' 'dense' 'densities' 'density' 'diffuse' 'discrete' 'distortion'\n",
      " 'eggshell' 'elevate' 'encapsulated' 'enlarged' 'entire' 'entirely'\n",
      " 'extending' 'fat' 'fatty' 'fibro' 'fibroadenoma' 'fibroadenomas'\n",
      " 'fibrofatty' 'fibroglandular' 'findings' 'fine' 'finidngs' 'foci' 'focus'\n",
      " 'fundings' 'glandular' 'half' 'high' 'higher' 'huge' 'hypodense' 'ill'\n",
      " 'illdefined' 'infiltrating' 'infiltration' 'inner' 'innerouter'\n",
      " 'internal' 'intramammary' 'invading' 'inverted' 'involved' 'involving'\n",
      " 'irregular' 'isodense' 'large' 'largest' 'lesion' 'lesionaccessory'\n",
      " 'lesionbirads' 'lesionbirads4a' 'lesions' 'level' 'likely' 'linear'\n",
      " 'linearbranching' 'lobulated' 'looking' 'low' 'lower' 'lowers'\n",
      " 'lymphadenopathy' 'macrolobulated' 'malignant' 'malinant' 'mammograms'\n",
      " 'margin' 'margins' 'may' 'maybe' 'mbreast' 'microcalcification'\n",
      " 'microcalcifications' 'mid' 'midcentral' 'middle' 'mild' 'moderate'\n",
      " 'mostly' 'multilayered' 'multiple' 'muscle' 'muscles' 'near' 'necrosis'\n",
      " 'needs' 'nipple' 'node' 'nodebirads' 'nodes' 'nodular' 'normal'\n",
      " 'normally' 'obscure' 'obscured' 'occupying' 'opacities' 'opacity' 'outer'\n",
      " 'oval' 'overlapping' 'overlying' 'parenchyma' 'parenchymaacr' 'part'\n",
      " 'partial' 'partially' 'parts' 'pathological' 'pectoral' 'periareolar'\n",
      " 'pins' 'pleomorphic' 'popcorn' 'portion' 'positioned' 'posterior'\n",
      " 'postoperative' 'predominantly' 'predominently' 'probably' 'punctate'\n",
      " 'quadrant' 'quadrantbirads' 'quadrantof' 'quadrants' 'region' 'regions'\n",
      " 'resulting' 'retracted' 'retraction' 'retro' 'retroareolar' 'rim' 'round'\n",
      " 'scar' 'scattered' 'seems' 'seen' 'sensitivity' 'shadow' 'shows'\n",
      " 'significant' 'skin' 'slight' 'slightly' 'slin' 'small' 'soft' 'solid'\n",
      " 'specks' 'speculations' 'spiculated' 'spiculation' 'spiculations'\n",
      " 'spicules' 'staple' 'suest' 'suets' 'sugest' 'suggest' 'suggested'\n",
      " 'suggesting' 'suggests' 'suspiciois' 'suspicious' 'tail' 'thickening'\n",
      " 'thin' 'three' 'throughout' 'tiny' 'tissue' 'two' 'ultrasound' 'upon'\n",
      " 'upper' 'upperlower' 'upto' 'variablesized' 'vascular' 'visualised'\n",
      " 'visualized' 'walled' 'well' 'welldefined' 'whole' 'within']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def read_reports(report_folder):\n",
    "    reports = []\n",
    "    for filename in os.listdir(report_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(report_folder, filename), 'r') as file:\n",
    "                reports.append(file.read())\n",
    "    return reports\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "def extract_vocabulary(reports):\n",
    "    preprocessed_reports = [preprocess_text(report) for report in reports]\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "    X = vectorizer.fit_transform(preprocessed_reports)\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    return vocabulary\n",
    "report_folder = \"D:\\\\DMID\\\\24522883\\\\Reports\\\\Reports\" \n",
    "reports = read_reports(report_folder)\n",
    "vocabulary = extract_vocabulary(reports)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\3647608841.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mtest_clip_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 92\u001b[0m, in \u001b[0;36mtest_clip_inference\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDMID\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m24522883\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDICOM Export\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimg-00310-00001.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m candidates \u001b[38;5;241m=\u001b[39m vocabulary[:\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m---> 92\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mclip_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_best_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContrastive matches:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, score \u001b[38;5;129;01min\u001b[39;00m matches:\n",
      "Cell \u001b[1;32mIn[54], line 75\u001b[0m, in \u001b[0;36mCLIPv02Inference.find_best_matches\u001b[1;34m(self, image_path, text_candidates, top_k)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_best_matches\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path, text_candidates, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Get normalized similarity scores\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Sort by score\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[54], line 31\u001b[0m, in \u001b[0;36mCLIPv02Inference.get_similarity\u001b[1;34m(self, image_path, text_list)\u001b[0m\n\u001b[0;32m     28\u001b[0m     text_list \u001b[38;5;241m=\u001b[39m [text_list]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Process all texts as a batch\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Get features\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     img_features, text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m     42\u001b[0m         image_tensor,\n\u001b[0;32m     43\u001b[0m         tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     44\u001b[0m         tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     45\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3055\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 3114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3117\u001b[0m     )\n\u001b[0;32m   3119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   3120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3123\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "class CLIPv02Inference:\n",
    "    def __init__(self, model_path, device=None, temperature=0.07):\n",
    "        self.temperature = temperature\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        \n",
    "        # Load model\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        checkpoints = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoints['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def get_similarity(self, image_path, text_list):\n",
    "        \"\"\"Calculate contrastive similarity scores between image and text list\"\"\"\n",
    "        # Process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Ensure text_list is a list of strings\n",
    "        if isinstance(text_list, str):\n",
    "            text_list = [text_list]\n",
    "        \n",
    "        # Process all texts as a batch\n",
    "        tokens = self.tokenizer(\n",
    "            text_list,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get features\n",
    "            img_features, text_features = self.model(\n",
    "                image_tensor,\n",
    "                tokens['input_ids'],\n",
    "                tokens['attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Normalize features\n",
    "            img_features = F.normalize(img_features, dim=-1)\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "            \n",
    "            # Calculate similarity matrix\n",
    "            logits = torch.matmul(img_features, text_features.T) / self.temperature\n",
    "            \n",
    "            # Calculate contrastive probabilities\n",
    "            probs = F.softmax(logits, dim=-1).squeeze()\n",
    "            \n",
    "            # Get individual scores using cross-entropy normalization\n",
    "            scores = []\n",
    "            for i, text in enumerate(text_list):\n",
    "                # Calculate positive pair score\n",
    "                pos_score = logits[0, i]\n",
    "                \n",
    "                # Calculate negative pairs score (other texts)\n",
    "                neg_indices = [j for j in range(len(text_list)) if j != i]\n",
    "                neg_scores = logits[0, neg_indices]\n",
    "                \n",
    "                # Normalized score using positive and negative pairs\n",
    "                normalized_score = (pos_score / (pos_score + neg_scores.sum())).item()\n",
    "                scores.append((text, normalized_score))\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def find_best_matches(self, image_path, text_candidates, top_k=5):\n",
    "        # Get normalized similarity scores\n",
    "        similarities = self.get_similarity(image_path, text_candidates)\n",
    "        \n",
    "        # Sort by score\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return similarities[:top_k]\n",
    "\n",
    "# Usage example\n",
    "def test_clip_inference():\n",
    "    clip_inference = CLIPv02Inference(\n",
    "        model_path=\"models/clip_v02_final-AugL-Drop-03.pth\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "    candidates = vocabulary[:100]\n",
    "    \n",
    "    matches = clip_inference.find_best_matches(image_path, candidates)\n",
    "    print(\"\\nContrastive matches:\")\n",
    "    for text, score in matches:\n",
    "        print(f\"Score: {score:.3f} | Text: {text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_clip_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\2262768396.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matches (CLIP-style):\n",
      "Score: 0.109 | Text: fibroadenoma\n",
      "Score: 0.102 | Text: hypodense\n",
      "Score: 0.056 | Text: adenopathies\n",
      "Score: 0.052 | Text: illdefined\n",
      "Score: 0.044 | Text: elevate\n"
     ]
    }
   ],
   "source": [
    "class CLIPv02Inference:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        \n",
    "        checkpoints = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoints['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def best_match(self, image_path, candidate_text, top_k=5):\n",
    "        try:\n",
    "            # Process image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            batch_size = 100\n",
    "            all_scores = []\n",
    "            \n",
    "            for i in range(0, len(candidate_text), batch_size):\n",
    "                batch_text = candidate_text[i:i + batch_size]\n",
    "                \n",
    "                # Tokenize text\n",
    "                tokens = self.tokenizer(\n",
    "                    batch_text,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Get features using single forward pass\n",
    "                    image_features, text_features = self.model(\n",
    "                        image_tensor,\n",
    "                        tokens['input_ids'],\n",
    "                        tokens['attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    # Normalize features\n",
    "                    image_features = F.normalize(image_features, dim=1)\n",
    "                    text_features = F.normalize(text_features, dim=1)\n",
    "                    \n",
    "                    # Compute similarity following CLIP paper\n",
    "                    logit_scale = self.logit_scale.exp()\n",
    "                    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "                    logits_per_text = logits_per_image.t()\n",
    "                    \n",
    "                    # Get similarity scores\n",
    "                    similarity = torch.nn.functional.softmax(logits_per_image, dim=1)\n",
    "                    \n",
    "                    for j, text in enumerate(batch_text):\n",
    "                        score = similarity[0, j].item()\n",
    "                        all_scores.append((text, score))\n",
    "            \n",
    "            all_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            return all_scores[:top_k]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "# Test\n",
    "clip_inference = CLIPv02Inference(\n",
    "    model_path=\"models/clip_v02_final-AugL-Drop-03.pth\"\n",
    ")\n",
    "\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "vocabulary = [str(x) for x in vocabulary[:100]]  # Ensure strings\n",
    "matches = clip_inference.best_match(image_path, vocabulary)\n",
    "\n",
    "print(\"\\nTop matches (CLIP-style):\")\n",
    "for text, score in matches:\n",
    "    print(f'Score: {score:.3f} | Text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\1139486608.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoints = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matches:\n",
      "Score: 0.087 | Text: fibroadenoma\n",
      "Score: 0.082 | Text: hypodense\n",
      "Score: 0.040 | Text: adenopathies\n",
      "Score: 0.035 | Text: illdefined\n",
      "Score: 0.023 | Text: elevate\n",
      "Score: -0.005 | Text: abnormality\n",
      "Score: -0.010 | Text: defied\n",
      "Score: -0.013 | Text: ill\n",
      "Score: -0.013 | Text: fibroadenomas\n",
      "Score: -0.027 | Text: fibroglandular\n"
     ]
    }
   ],
   "source": [
    "class CLIPv02Inference:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
    "        self.model = CLIPv02(embed_dim=1024)\n",
    "        checkpoints = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoints['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def compute_similarity(self, img_features, text_features):\n",
    "        # Normalize features\n",
    "        img_features = F.normalize(img_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = torch.matmul(img_features, text_features.t())\n",
    "        return similarity\n",
    "\n",
    "    def best_match(self, image_path, candidate_text, top_k=10):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            batch_size = 100\n",
    "            all_scores = []\n",
    "            \n",
    "            for i in range(0, len(candidate_text), batch_size):\n",
    "                batch_text = candidate_text[i:i + batch_size]\n",
    "                \n",
    "                tokens = self.tokenizer(\n",
    "                    batch_text,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Get features\n",
    "                    image_features, text_features = self.model(\n",
    "                        image_tensor,\n",
    "                        tokens['input_ids'],\n",
    "                        tokens['attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    # Compute similarity scores\n",
    "                    similarity = self.compute_similarity(image_features, text_features)\n",
    "                    \n",
    "                    # Get raw similarity scores\n",
    "                    for j, text in enumerate(batch_text):\n",
    "                        score = similarity[0, j].item()\n",
    "                        all_scores.append((text, score))\n",
    "            \n",
    "            all_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            return all_scores[:top_k]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "# Test\n",
    "clip_inference = CLIPv02Inference(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "matches = clip_inference.best_match(image_path, vocabulary)\n",
    "\n",
    "print(\"\\nTop matches:\")\n",
    "for text, score in matches:\n",
    "    print(f'Score: {score:.3f} | Text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.087 | Text: fibroadenoma\n",
      "Score: 0.086 | Text: hypodense\n",
      "Score: 0.043 | Text: elevate\n",
      "Score: 0.037 | Text: illdefined\n",
      "Score: 0.029 | Text: adenopathies\n",
      "Score: 0.000 | Text: defied\n",
      "Score: -0.013 | Text: fibroadenomas\n",
      "Score: -0.027 | Text: fibroglandular\n",
      "Score: -0.051 | Text: ill\n",
      "Score: -0.063 | Text: abnormality\n",
      "Score: -0.067 | Text: asymetric\n",
      "Score: -0.071 | Text: finidngs\n",
      "Score: -0.072 | Text: aspects\n",
      "Score: -0.074 | Text: amorphous\n",
      "Score: -0.075 | Text: compositions\n",
      "Score: -0.079 | Text: breastbirads\n",
      "Score: -0.080 | Text: chemoport\n",
      "Score: -0.086 | Text: adenopathy\n",
      "Score: -0.087 | Text: acrb\n",
      "Score: -0.087 | Text: composition\n",
      "Score: -0.090 | Text: benignlooking\n",
      "Score: -0.094 | Text: fine\n",
      "Score: -0.097 | Text: fibrofatty\n",
      "Score: -0.098 | Text: acrc\n",
      "Score: -0.100 | Text: discrete\n",
      "Score: -0.101 | Text: acrd\n",
      "Score: -0.104 | Text: architectural\n",
      "Score: -0.105 | Text: birads\n",
      "Score: -0.107 | Text: infiltrating\n",
      "Score: -0.108 | Text: benin\n",
      "Score: -0.109 | Text: defined\n",
      "Score: -0.110 | Text: fibro\n",
      "Score: -0.115 | Text: 23\n",
      "Score: -0.125 | Text: bening\n",
      "Score: -0.129 | Text: acr\n",
      "Score: -0.131 | Text: entire\n",
      "Score: -0.132 | Text: central\n",
      "Score: -0.133 | Text: focus\n",
      "Score: -0.134 | Text: fatty\n",
      "Score: -0.135 | Text: area\n",
      "Score: -0.138 | Text: correlation\n",
      "Score: -0.138 | Text: axilla\n",
      "Score: -0.139 | Text: high\n",
      "Score: -0.139 | Text: 4b\n",
      "Score: -0.141 | Text: findings\n",
      "Score: -0.145 | Text: acra\n",
      "Score: -0.145 | Text: fundings\n",
      "Score: -0.147 | Text: enlarged\n",
      "Score: -0.147 | Text: brest\n",
      "Score: -0.148 | Text: density\n",
      "Score: -0.148 | Text: aspect\n",
      "Score: -0.148 | Text: appear\n",
      "Score: -0.149 | Text: causing\n",
      "Score: -0.150 | Text: entirely\n",
      "Score: -0.153 | Text: benign\n",
      "Score: -0.153 | Text: huge\n",
      "Score: -0.153 | Text: calcification\n",
      "Score: -0.154 | Text: another\n",
      "Score: -0.156 | Text: commented\n",
      "Score: -0.160 | Text: anterior\n",
      "Score: -0.160 | Text: densities\n",
      "Score: -0.161 | Text: abnormal\n",
      "Score: -0.161 | Text: adjacent\n",
      "Score: -0.161 | Text: benignvascular\n",
      "Score: -0.162 | Text: coarse\n",
      "Score: -0.162 | Text: dense\n",
      "Score: -0.163 | Text: appears\n",
      "Score: -0.164 | Text: 23rd\n",
      "Score: -0.165 | Text: half\n",
      "Score: -0.168 | Text: diffuse\n",
      "Score: -0.168 | Text: bifid\n",
      "Score: -0.170 | Text: calcified\n",
      "Score: -0.170 | Text: beninglooking\n",
      "Score: -0.170 | Text: extending\n",
      "Score: -0.171 | Text: distortion\n",
      "Score: -0.172 | Text: branching\n",
      "Score: -0.176 | Text: 4a\n",
      "Score: -0.177 | Text: birads3\n",
      "Score: -0.178 | Text: fat\n",
      "Score: -0.179 | Text: birads4b\n",
      "Score: -0.185 | Text: could\n",
      "Score: -0.185 | Text: higher\n",
      "Score: -0.185 | Text: asymmetric\n",
      "Score: -0.186 | Text: biards\n",
      "Score: -0.190 | Text: appearance\n",
      "Score: -0.192 | Text: 4c\n",
      "Score: -0.198 | Text: breast\n",
      "Score: -0.199 | Text: beingn\n",
      "Score: -0.199 | Text: birads4c\n",
      "Score: -0.202 | Text: birads5\n",
      "Score: -0.202 | Text: axillary\n",
      "Score: -0.203 | Text: foci\n",
      "Score: -0.205 | Text: abovementioned\n",
      "Score: -0.207 | Text: areolar\n",
      "Score: -0.209 | Text: 4cno\n",
      "Score: -0.215 | Text: encapsulated\n",
      "Score: -0.217 | Text: calcifications\n",
      "Score: -0.230 | Text: glandular\n",
      "Score: -0.237 | Text: birads4a\n",
      "Score: -0.274 | Text: eggshell\n"
     ]
    }
   ],
   "source": [
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "candidate_text = vocabulary[:100]\n",
    "matches = clip_inference.best_match(image_path, candidate_text)\n",
    "for text, score in matches:\n",
    "    print(f'Score: {score:.3f} | Text: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\4195471043.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\",\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def load_model_safely():\n",
    "    try:\n",
    "        # Check CUDA availability first\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"CUDA is not available. Using CPU...\")\n",
    "            device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            # Reset CUDA device\n",
    "            torch.cuda.init()\n",
    "            torch.cuda.empty_cache()\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "        # Set environment variables\n",
    "        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "        \n",
    "        # Initialize model on CPU first\n",
    "        model = CLIPv02(embed_dim=1024)\n",
    "        \n",
    "        # Load checkpoint to CPU first\n",
    "        checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\", \n",
    "                              map_location='cpu')\n",
    "        \n",
    "        # Load state dict\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Move to appropriate device\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model, device\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"CUDA Error: {str(e)}\")\n",
    "        print(\"Falling back to CPU...\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = CLIPv02(embed_dim=1024)\n",
    "        checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\", \n",
    "                              map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        return model, device\n",
    "\n",
    "# Usage\n",
    "model, device = load_model_safely()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_13392\\1617374525.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "ERROR:__main__:Execution failed: Failed to load model: 'collections.OrderedDict' object has no attribute 'to'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load model: 'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 45\u001b[0m, in \u001b[0;36mMedicalImageTextMatcher.__init__\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m checkpoint\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 126\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;66;03m# Initialize with model path\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m         matcher \u001b[38;5;241m=\u001b[39m \u001b[43mMedicalImageTextMatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/clip_v02_final-AugL-Drop-03.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m# Test model loading\u001b[39;00m\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 49\u001b[0m, in \u001b[0;36mMedicalImageTextMatcher.__init__\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to load model: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Setup logging and device\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "class MedicalImageTextMatcher:\n",
    "    def __init__(self, checkpoint_path):\n",
    "        # Load and validate checkpoint\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            \n",
    "            # Handle different checkpoint structures\n",
    "            if isinstance(checkpoint, dict):\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    self.model = checkpoint['model_state_dict']\n",
    "                elif 'model' in checkpoint:\n",
    "                    self.model = checkpoint['model']\n",
    "                else:\n",
    "                    # Assume checkpoint is the model state dict itself\n",
    "                    self.model = checkpoint\n",
    "            else:\n",
    "                # Checkpoint is the model itself\n",
    "                self.model = checkpoint\n",
    "                \n",
    "            self.model = self.model.to(device)\n",
    "            self.model.eval()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    # Rest of the class remains the same...\n",
    "        \n",
    "    def extract_vocabulary(self, reports_folder, min_freq=5):\n",
    "        vocab = Counter()\n",
    "        reports_path = Path(reports_folder)\n",
    "        \n",
    "        for file_path in tqdm(list(reports_path.glob('*.txt')), desc=\"Processing reports\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = text.split()\n",
    "                    vocab.update(words)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Skipped {file_path}: {str(e)}\")\n",
    "                \n",
    "        return [word for word, count in vocab.items() if count >= min_freq]\n",
    "\n",
    "    def process_batch(self, image_tensor, texts, batch_size=32):\n",
    "        all_similarities = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                tokens = tokenizer(\n",
    "                    batch_texts,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=128,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    img_features, text_features = self.model(\n",
    "                        image_tensor, \n",
    "                        tokens['input_ids'],\n",
    "                        tokens['attention_mask']\n",
    "                    )\n",
    "                    \n",
    "                    img_features = F.normalize(img_features, dim=-1)\n",
    "                    text_features = F.normalize(text_features, dim=-1)\n",
    "                    \n",
    "                    similarity = torch.matmul(img_features, text_features.T)\n",
    "                    all_similarities.append(similarity)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch processing error: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return torch.cat(all_similarities, dim=1) if all_similarities else None\n",
    "\n",
    "    def analyze_image(self, image_path, vocab, top_k=20):\n",
    "        try:\n",
    "            # Process image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get similarities\n",
    "            similarities = self.process_batch(image_tensor, vocab)\n",
    "            if similarities is None:\n",
    "                return []\n",
    "                \n",
    "            # Get top matches\n",
    "            scores = similarities[0]\n",
    "            top_scores, top_indices = scores.topk(min(top_k, len(vocab)))\n",
    "            \n",
    "            return [(vocab[idx], score.item()) for score, idx in zip(top_scores, top_indices)]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis error: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize with model path\n",
    "        matcher = MedicalImageTextMatcher(\"models/clip_v02_final-AugL-Drop-03.pth\")\n",
    "        \n",
    "        # Test model loading\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "        \n",
    "        # Continue with analysis...\n",
    "        vocab = matcher.extract_vocabulary(\"D:\\\\DMID\\\\24522883\\\\Reports\\\\Reports\")\n",
    "        image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "        matches = matcher.analyze_image(image_path, vocab)\n",
    "        \n",
    "        print(\"\\nTop matches:\")\n",
    "        for term, score in matches:\n",
    "            print(f\"{term}: {score:.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Execution failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Output Features (Top 5):\n",
      "[228.35013 130.38225 106.90538 100.50372 100.14967]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_clip_output(model, image_path, device='cuda'):\n",
    "    # Preprocessing pipeline\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            features = model.image_encoder(image_tensor)\n",
    "            # Get top feature values\n",
    "            top_values, _ = torch.topk(features.squeeze(), k=5)\n",
    "            print(\"\\nModel Output Features (Top 5):\")\n",
    "            print(top_values.cpu().numpy())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Usage\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "get_clip_output(model, image_path)\n",
    "# Add visualization to see which features are most activated\n",
    "def visualize_features(features):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(5), features)\n",
    "    plt.title('Top 5 Feature Activations')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Activation Strength')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "# visualize_features([228.35013, 130.38225, 106.90538, 100.50372, 100.14967])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matching words:\n",
      "- nodule: -0.062\n",
      "- malignant: -0.076\n",
      "- round: -0.078\n",
      "- spiculated: -0.104\n",
      "- scattered: -0.109\n",
      "\n",
      "Top matching features:\n",
      "- focal asymmetry: 1.000\n",
      "- benign finding: 0.964\n",
      "- suspicious mass: 0.763\n",
      "- fibroglandular tissue: 0.557\n",
      "- architectural distortion: 0.544\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def extract_vocabulary(reports_folder):\n",
    "    \"\"\"Extract vocabulary from medical reports\"\"\"\n",
    "    vocab = Counter()\n",
    "    for filename in os.listdir(reports_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(reports_folder, filename), 'r') as f:\n",
    "                text = f.read().lower()\n",
    "                # Split into words and count\n",
    "                words = text.split()\n",
    "                vocab.update(words)\n",
    "    \n",
    "    # Filter common medical terms (frequency > 5)\n",
    "    medical_vocab = [word for word, count in vocab.items() if count > 5]\n",
    "    return medical_vocab\n",
    "\n",
    "def match_image_to_vocab(model, image_path, vocab, device='cuda'):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Process vocabulary\n",
    "    text_inputs = tokenizer(vocab, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_features, text_features = model(\n",
    "            image_tensor,\n",
    "            text_inputs['input_ids'],\n",
    "            text_inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = F.cosine_similarity(\n",
    "            img_features.unsqueeze(1),\n",
    "            text_features.unsqueeze(0),\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # Get top matches\n",
    "        top_scores, top_indices = similarities[0].topk(10)\n",
    "        \n",
    "        print(\"\\nTop matching terms from reports:\")\n",
    "        for score, idx in zip(top_scores, top_indices):\n",
    "            print(f\"- {vocab[idx]}: {score:.3f}\")\n",
    "\n",
    "# Usage\n",
    "reports_folder = \"path/to/reports/folder\"\n",
    "vocab = extract_vocabulary(reports_folder)\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "match_image_to_vocab(model, image_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matching terms from reports:\n",
      "- fibroadenoma: 0.081\n",
      "- (acr: 0.025\n",
      "- abnormality: 0.007\n",
      "- findings:: -0.004\n",
      "- may: -0.006\n",
      "- 3).: -0.012\n",
      "- 2).: -0.013\n",
      "- lobulated: -0.016\n",
      "- adenopathy: -0.028\n",
      "- 2): -0.028\n",
      "['breast', 'composition:', 'predominantly', 'fibro', 'fatty', 'parenchyma', '(acr', 'b)', 'birads:', '3', 'and', '5', 'findings:', 'irregular', 'ill-defined', 'soft', 'opacity', 'with', 'microcalcifications', 'suggests', 'malignant', 'lesion', 'small', 'well', 'defined', 'nodular', 'benign', '(birads-3)', 'vascular', 'calcifications', 'skin', 'nipple', '-', 'no', 'abnormality', 'axillary', 'adenopathy', 'scattered', 'glandular', '1', 'abnormal', 'looking', 'a)', 'well-defined', 'suggest', 'c)', 'oval', 'opacities', 'in', 'retroareolar', 'upper', 'quadrant', 'lesions', 'outer', '4b', 'spiculated', 'margin', 'the', 'suspicious', '4c', '4a', 'a', 'large', 'partially', 'seen', 'low', 'region', 'to', 'be', 'appears', 'benign-looking', 'adenopathy.', 'moderate', 'causing', 'architectural', 'distortion', 'popcorn', 'calcification', 'lobulated', 'inner', 'appear', 'lower', 'periareolar', 'likely', '(birads', '3).', 'are', 'diffuse', 'thickening', 'is', 'seen.', 'foci', 'of', 'microcalcification', 'quadrant.', 'few', 'adjacent', 'involving', 'overlying', 'another', 'skin,', 'nipple,', 'pectoral', 'muscle', 'mid', 'part', '4b).', 'normal.', 'significant', '4b)', 'd)', 'opacity.', 'd).', 'b).', '3,', '4a)', 'tiny', 'an', '4a).', '3)', 'a).', 'high', '4c)', 'dense', 'c).', 'two', 'breast.', 'within', 'lesion.', 'may', 'discrete', 'spiculations', 'retraction', '5)', 'multiple', 'or', '4c).', 'margins', 'probably', 'mild', 'node', 'bening', 'central', 'coarse', 'node.', 'microcalcification.', 'enlarged', '5).', 'not', 'visualized', 'posterior', 'aspect', '2', 'anterior', '2)', '2).', 'nodes.', '(acr-b).', 'predominently', 'extending', 'upto', 'internal', 'nodes', 'amorphous', 'calcified', 'about', '(acr-a).', 'fibroadenoma']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def extract_vocabulary(reports_folder):\n",
    "    \"\"\"Extract vocabulary from medical reports\"\"\"\n",
    "    vocab = Counter()\n",
    "    for filename in os.listdir(reports_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(reports_folder, filename), 'r') as f:\n",
    "                text = f.read().lower()\n",
    "                # Split into words and count\n",
    "                words = text.split()\n",
    "                vocab.update(words)\n",
    "    \n",
    "    # Filter common medical terms (frequency > 5)\n",
    "    medical_vocab = [word for word, count in vocab.items() if count > 5]\n",
    "    return medical_vocab\n",
    "\n",
    "def match_image_to_terms_normalized(model, image_path, vocab, device='cuda'):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    temperature = 0.07  # Temperature scaling factor\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        text_inputs = tokenizer(vocab, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            img_features, text_features = model(\n",
    "                image_tensor,\n",
    "                text_inputs['input_ids'],\n",
    "                text_inputs['attention_mask']\n",
    "            )\n",
    "            \n",
    "            # Ensure L2 normalization\n",
    "            img_features = F.normalize(img_features, dim=-1)\n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "            \n",
    "            # Calculate similarity with temperature scaling\n",
    "            similarities = torch.mm(img_features, text_features.t()) / temperature\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(similarities, dim=-1)\n",
    "            \n",
    "            # Get absolute values and normalize to [0,1]\n",
    "            scores = torch.abs(similarities[0])\n",
    "            scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "            \n",
    "            top_scores, top_indices = scores.topk(10)\n",
    "            \n",
    "            print(\"\\nTop matching terms (normalized):\")\n",
    "            for score, idx in zip(top_scores, top_indices):\n",
    "                print(f\"- {vocab[idx]}: {score:.3f}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Usage\n",
    "reports_folder = \"D:\\\\DMID\\\\24522883\\\\Reports\\\\Reports\"\n",
    "vocab = extract_vocabulary(reports_folder)\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "match_image_to_vocab(model, image_path, vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_23092\\389256377.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPv02(\n",
       "  (image_encoder): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (image_proj): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (text_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the model\n",
    "model = CLIPv02(embed_dim=1024)\n",
    "checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 94.13514137268066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "text = \"\"\"\n",
    "BREAST COMPOSITION: \n",
    "fibro fatty and glandular breast parenchyma (ACR C)\n",
    "\n",
    "BIRADS: 3\n",
    "\n",
    "FINDINGS: \n",
    "few overlapping lobulated soft opacities seen in the outer quadrant of the breast suggest probably benign lesions (BIRADS 3). \n",
    "\n",
    "skin and nipple appear normal. \n",
    "\n",
    "no abnormal microcalcification.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "text_input_ids = inputs['input_ids'].to(device)\n",
    "attn_mask = (text_input_ids > 0).to(device)\n",
    "\n",
    "#  inference\n",
    "with torch.no_grad():\n",
    "    img_features, text_features = model(image, text_input_ids, attn_mask)\n",
    "\n",
    "# similarity score\n",
    "similarity_score = torch.matmul(img_features, text_features.T).item()\n",
    "print(f\"Similarity Score: {similarity_score*100}\")\n",
    "# Similarity Score: 94.13514137268066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_15160\\1988328628.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n",
      "c:\\Users\\Karim Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: -8.002786338329315\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import transforms\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "model = CLIPv02(embed_dim=1024)\n",
    "checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Text tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"\"\"\n",
    "BREAST COMPOSITION: \n",
    "fibro fatty and glandular breast parenchyma (ACR C)\n",
    "\n",
    "BIRADS: 3\n",
    "\n",
    "FINDINGS: \n",
    "few overlapping lobulated soft opacities seen in the outer quadrant of the breast suggest probably benign lesions (BIRADS 3). \n",
    "\n",
    "skin and nipple appear normal. \n",
    "\n",
    "no abnormal microcalcification.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "image = Image.open(\"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\")\n",
    "image = image_transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "input_ids = tokenized_text['input_ids'].to(device)\n",
    "attn_mask = tokenized_text['attention_mask'].to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_features, text_features = model(image, input_ids, attn_mask)\n",
    "\n",
    "similarity = torch.nn.functional.cosine_similarity(img_features, text_features)\n",
    "print(f\"Similarity: {similarity.item()*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Karim\n",
      "[nltk_data]     Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_15160\\164692508.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m extract_vocabulary(reports)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Extract the most relevant keywords\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m best_match_indices \u001b[38;5;241m=\u001b[39m similarity_scores\u001b[38;5;241m.\u001b[39margsort(descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    103\u001b[0m top_keywords \u001b[38;5;241m=\u001b[39m [vocabulary[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m best_match_indices[:\u001b[38;5;241m20\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[6], line 56\u001b[0m, in \u001b[0;36mextract_keywords\u001b[1;34m(image_path, texts)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_keywords\u001b[39m(image_path, texts):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Preprocess the image\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m(Image\u001b[38;5;241m.\u001b[39mopen(image_path))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Tokenize the texts\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-B-32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from duckdb import checkpoint\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import ollama\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the fine-tuned CLIP model\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "# model.load_state_dict(torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\fine_tuned_clip_dmid.pth\"))\n",
    "# model.eval()\n",
    "model = CLIPv02(embed_dim=1024)\n",
    "checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to read all reports from a folder\n",
    "def read_reports(report_folder):\n",
    "    reports = []\n",
    "    for filename in os.listdir(report_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(report_folder, filename), 'r') as file:\n",
    "                reports.append(file.read())\n",
    "    return reports\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to extract important vocabulary using TF-IDF\n",
    "def extract_vocabulary(reports):\n",
    "    preprocessed_reports = [preprocess_text(report) for report in reports]\n",
    "    vectorizer = TfidfVectorizer(max_features=100)  # Adjust max_features as needed\n",
    "    X = vectorizer.fit_transform(preprocessed_reports)\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    return vocabulary\n",
    "\n",
    "# Function to preprocess and encode images and texts\n",
    "def extract_keywords(image_path, texts):\n",
    "    # Preprocess the image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    text_tokens = tokenizer(texts).to(device)\n",
    "    \n",
    "    # Encode the image and texts\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        \n",
    "        # Normalize the features\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Function to generate a report using the Ollama model\n",
    "def generate_report_ollama(keywords):\n",
    "    input_text = \"\"\"Generate a medical report: \"Breast Composition, BIRADS, and Findings:\". based on the following keywords: \"\"\" + \", \".join(keywords)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': input_text\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model='gemma2:2b', messages=messages)\n",
    "    report = response['message']['content']\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Example usage\n",
    "report_folder = \"D:\\\\DMID\\\\24522883\\\\Reports\\\\Reports\"  # Replace with your reports folder path\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"  # Replace with your image path\n",
    "\n",
    "# Read and preprocess reports\n",
    "reports = read_reports(report_folder)\n",
    "vocabulary = extract_vocabulary(reports)\n",
    "\n",
    "# Extract the most relevant keywords\n",
    "similarity_scores = extract_keywords(image_path, vocabulary)\n",
    "best_match_indices = similarity_scores.argsort(descending=True).squeeze().tolist()\n",
    "top_keywords = [vocabulary[i] for i in best_match_indices[:20]]\n",
    "print(top_keywords)  # Select top 10 keywords\n",
    "\n",
    "# Generate the report based on the top keywords\n",
    "generated_report = generate_report_ollama(top_keywords)\n",
    "print(\"\\nGenerated Report:\")\n",
    "print(generated_report)\n",
    "#['nipple', 'breast', 'fatty', 'abnormal', 'soft', 'oval', 'mild', 'skin', 'normal', 'abnormality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Karim\n",
      "[nltk_data]     Mohamed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "C:\\Users\\Karim Mohamed\\AppData\\Local\\Temp\\ipykernel_2484\\970893607.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords: ['microcalcifications', 'calcifications', 'abnormality', 'opacity', 'retraction', 'spiculated', 'axillary', 'parenchyma', 'periareolar', 'glandular', 'lesion', 'adenopathy', 'microcalcification', 'foci', 'opacities', 'calcification', 'benignlooking', 'birads3', 'birads', 'pectoral']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import ollama\n",
    "from transformers import BertTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize model and load checkpoint\n",
    "model = CLIPv02(embed_dim=1024)\n",
    "checkpoint = torch.load(\"G:\\\\PYTHON\\\\open_clip\\\\models\\\\clip_v02_final-AugL-Drop-03.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize tokenizer and preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def read_reports(report_folder):\n",
    "    reports = []\n",
    "    for filename in os.listdir(report_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(report_folder, filename), 'r') as file:\n",
    "                reports.append(file.read())\n",
    "    return reports\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def extract_vocabulary(reports):\n",
    "    preprocessed_reports = [preprocess_text(report) for report in reports]\n",
    "    vectorizer = TfidfVectorizer(max_features=100)\n",
    "    X = vectorizer.fit_transform(preprocessed_reports)\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    return vocabulary\n",
    "\n",
    "# Function to extract keywords using correct model methods\n",
    "def extract_keywords(image_path, texts):\n",
    "    # Ensure texts is list\n",
    "    if not isinstance(texts, list):\n",
    "        texts = texts.tolist()\n",
    "    \n",
    "    # Preprocess image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Process texts in batches\n",
    "    all_text_features = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        # Tokenize\n",
    "        text_tokens = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get text embeddings using forward pass\n",
    "        with torch.no_grad():\n",
    "            # Use forward pass instead of encode_text\n",
    "            text_output = model.text_encoder(text_tokens.input_ids)[1]  # Get pooled output\n",
    "            text_features = model.text_proj(text_output)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            all_text_features.append(text_features)\n",
    "    \n",
    "    # Stack text features\n",
    "    text_features = torch.cat(all_text_features, dim=0)\n",
    "    \n",
    "    # Get image features using forward pass\n",
    "    with torch.no_grad():\n",
    "        # Use forward pass instead of encode_image\n",
    "        image_output = model.image_encoder(image)\n",
    "        image_features = model.image_proj(image_output)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def generate_report_ollama(keywords):\n",
    "    input_text = \"\"\"Generate a medical report: \"Breast Composition, BIRADS, and Findings:\". based on the following keywords: \"\"\" + \", \".join(keywords)\n",
    "    messages = [{'role': 'user', 'content': input_text}]\n",
    "    response = ollama.chat(model='gemma2:2b', messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example usage\n",
    "report_folder = \"D:\\\\DMID\\\\24522883\\\\Reports\\\\Reports\"\n",
    "image_path = \"D:\\\\DMID\\\\24522883\\\\DICOM Export\\\\img-00310-00001.jpg\"\n",
    "\n",
    "# Process reports and extract keywords\n",
    "reports = read_reports(report_folder)\n",
    "vocabulary = extract_vocabulary(reports)\n",
    "\n",
    "# Get keywords\n",
    "similarity_scores = extract_keywords(image_path, vocabulary)\n",
    "best_match_indices = similarity_scores.argsort(descending=True).squeeze().tolist()\n",
    "top_keywords = [vocabulary[i] for i in best_match_indices[:20]]\n",
    "print(\"Top Keywords:\", top_keywords)\n",
    "\n",
    "# Generate report\n",
    "# generated_report = generate_report_ollama(top_keywords)\n",
    "# print(\"\\nGenerated Report:\")\n",
    "# print(generated_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
